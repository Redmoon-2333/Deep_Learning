# 第 2 章 神经网络的学习

## 2.1 损失函数

损失函数（Loss Function）是衡量神经网络预测值与真实值之间差距的指标，是神经网络学习的核心。通过最小化损失函数，网络可以不断调整参数，提高预测精度。

### 2.1.1 常见损失函数

损失函数的选择直接影响模型的训练效果和收敛速度。以下是几种常见的损失函数：

| 损失函数 | 适用任务 | 特点 |
|---------|---------|------|
| 均方误差（MSE） | 回归 | 对离群点敏感 |
| 平均绝对误差（MAE） | 回归 | 对离群点鲁棒 |
| 交叉熵损失 | 分类 | 适合概率输出 |
| Hinge Loss | 分类 | SVM常用 |

**选择原则**：
- 回归问题：优先考虑 MSE 或 MAE
- 分类问题：优先考虑交叉熵损失
- 特殊需求：根据数据分布和任务特点选择

### 2.1.2 分类任务损失函数

分类任务的目标是将输入数据划分到不同的类别中。

#### 交叉熵损失（Cross-Entropy Loss）

交叉熵损失是分类任务中最常用的损失函数，用于衡量预测概率分布与真实分布之间的差异。

**二分类交叉熵**：
```
L = -[y·log(p) + (1-y)·log(1-p)]
```

其中：
- y：真实标签（0或1）
- p：预测为正类的概率

**多分类交叉熵**：
```
L = -Σᵢ yᵢ·log(pᵢ)
```

其中：
- yᵢ：真实标签的one-hot编码
- pᵢ：预测为第i类的概率

**代码实现**：
```python
# TODO: 待补充交叉熵损失函数实现
```

**特点**：
- 预测越接近真实值，损失越小
- 对错误预测惩罚较大
- 与Softmax配合使用效果最佳

### 2.1.3 回归任务损失函数

回归任务的目标是预测连续数值。

#### 均方误差（Mean Squared Error, MSE）

**定义**：
```
MSE = (1/n) Σᵢ (yᵢ - ŷᵢ)²
```

其中：
- yᵢ：真实值
- ŷᵢ：预测值
- n：样本数量

**代码实现**：
```python
# TODO: 待补充MSE损失函数实现
```

**特点**：
- 对离群点敏感（平方放大误差）
- 可微分，便于优化
- 梯度随误差线性变化

#### 平均绝对误差（Mean Absolute Error, MAE）

**定义**：
```
MAE = (1/n) Σᵢ |yᵢ - ŷᵢ|
```

**特点**：
- 对离群点更鲁棒
- 在零点处不可微

#### MSE vs MAE 对比

| 特性 | MSE | MAE |
|-----|-----|-----|
| 离群点敏感度 | 高 | 低 |
| 可微性 | 处处可微 | 零点不可微 |
| 梯度特点 | 随误差变化 | 恒定 |
| 收敛速度 | 较快 | 较慢 |

---

## 2.2 数值微分

数值微分是通过数值方法近似计算函数导数的技术，是理解神经网络梯度计算的基础。

### 2.2.1 导数和数值微分

#### 导数的定义

导数表示函数在某一点的瞬时变化率：

```
f'(x) = lim[h→0] (f(x+h) - f(x)) / h
```

#### 数值微分

由于计算机无法处理无穷小量，我们使用有限差分来近似导数：

**前向差分**：
```
f'(x) ≈ (f(x+h) - f(x)) / h
```

**中心差分**（推荐，精度更高）：
```
f'(x) ≈ (f(x+h) - f(x-h)) / 2h
```

**代码实现**：
```python
# TODO: 待补充数值微分实现
```

**注意事项**：
- h 的选择：通常取 1e-4 到 1e-7
- h 太大：截断误差大
- h 太小：舍入误差大

### 2.2.2 偏导数

对于多变量函数，偏导数表示函数关于某一个变量的变化率，其他变量保持不变。

**定义**：
对于函数 f(x₁, x₂, ..., xₙ)，关于 xᵢ 的偏导数为：

```
∂f/∂xᵢ = lim[h→0] (f(x₁,...,xᵢ+h,...,xₙ) - f(x₁,...,xᵢ,...,xₙ)) / h
```

**示例**：
对于 f(x₀, x₁) = x₀² + x₁²

```
∂f/∂x₀ = 2x₀
∂f/∂x₁ = 2x₁
```

**代码实现**：
```python
# TODO: 待补充偏导数计算实现
```

### 2.2.3 梯度

梯度是由所有偏导数组成的向量，指向函数值增加最快的方向。

**定义**：
```
∇f = (∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ)
```

**梯度的性质**：
1. **方向**：指向函数值上升最快的方向
2. **大小**：表示最大变化率
3. **负梯度**：指向函数值下降最快的方向

**代码实现**：
```python
# TODO: 待补充梯度计算实现
```

**几何意义**：
- 在等高线图中，梯度垂直于等高线
- 梯度的模表示函数变化的剧烈程度

---

## 2.3 神经网络的梯度计算

神经网络的训练需要计算损失函数关于所有参数的梯度，用于更新网络权重。

### 梯度计算方法

1. **数值微分法**：
   - 原理：使用有限差分近似导数
   - 优点：实现简单，可用于验证
   - 缺点：计算量大，精度有限

2. **解析法（反向传播）**：
   - 原理：利用链式法则计算梯度
   - 优点：计算效率高，精度高
   - 缺点：实现较复杂

### 神经网络梯度的数值计算

**代码实现**：
```python
# TODO: 待补充神经网络梯度计算实现
```

### 梯度检验

用数值梯度验证解析梯度的正确性：

```
相对误差 = |数值梯度 - 解析梯度| / max(|数值梯度|, |解析梯度|)
```

- 相对误差 < 1e-7：梯度计算正确
- 相对误差 > 1e-5：可能存在问题

---

## 2.4 随机梯度下降法（SGD）

### 2.4.1 梯度下降法

梯度下降法是最基本的优化算法，通过沿负梯度方向迭代更新参数来最小化损失函数。

**更新公式**：
```
θ = θ - η·∇L(θ)
```

其中：
- θ：模型参数
- η：学习率（learning rate）
- ∇L(θ)：损失函数关于参数的梯度

**梯度下降的变体**：

| 方法 | 描述 | 特点 |
|-----|------|------|
| 批量梯度下降（BGD） | 使用全部数据计算梯度 | 稳定但慢 |
| 随机梯度下降（SGD） | 使用单个样本计算梯度 | 快但不稳定 |
| 小批量梯度下降（Mini-batch） | 使用一小批数据 | 平衡稳定性和速度 |

**代码实现**：
```python
# TODO: 待补充梯度下降法实现
```

### 2.4.2 模型训练相关概念

#### Epoch（轮次）

一个 epoch 表示所有训练数据被完整使用一次。

```
训练过程：Epoch 1 → Epoch 2 → ... → Epoch N
```

#### Batch Size（批量大小）

每次更新参数时使用的样本数量。

**选择建议**：
- 常用值：32, 64, 128, 256
- 内存限制：根据GPU显存调整
- 收敛性：较大的batch更稳定，较小的batch噪声更大但可能找到更好的解

#### Iteration（迭代次数）

```
迭代次数 = 训练样本数 / Batch Size
```

**示例**：
- 训练集：10000个样本
- Batch Size：100
- 每个Epoch的迭代次数：10000 / 100 = 100次

#### 学习率（Learning Rate）

学习率决定了参数更新的步长。

**学习率的影响**：
- 太大：可能跳过最优解，导致震荡或发散
- 太小：收敛速度慢
- 合适：稳定收敛到最优解

**学习率调整策略**：
1. 固定学习率
2. 学习率衰减（Step Decay, Exponential Decay）
3. 自适应学习率（Adam, RMSprop）

### 2.4.3 SGD

随机梯度下降（Stochastic Gradient Descent）是最基础的优化算法。

**算法流程**：
```
1. 初始化参数 θ
2. 重复以下步骤直到收敛：
   a. 随机选择一个（或一批）样本
   b. 计算损失函数关于参数的梯度
   c. 更新参数：θ = θ - η·∇L(θ)
```

**代码实现**：
```python
# TODO: 待补充SGD优化器实现
```

**SGD的优缺点**：

| 优点 | 缺点 |
|-----|------|
| 实现简单 | 学习率难以选择 |
| 计算效率高 | 容易陷入局部最优 |
| 在线学习 | 在鞍点处收敛慢 |
| 有助于跳出局部最优 | 震荡现象 |

**SGD的改进版本**：
1. **动量（Momentum）**：引入历史梯度信息
2. **Nesterov动量**：预测下一步位置后再计算梯度
3. **AdaGrad**：自适应调整每个参数的学习率
4. **RMSprop**：解决AdaGrad学习率衰减过快问题
5. **Adam**：结合动量和自适应学习率

---

## 2.5 综合代码实现

本节将整合以上所有概念，实现一个完整的神经网络训练流程。

### 训练流程概览

```
1. 数据准备
   ↓
2. 网络初始化
   ↓
3. 前向传播（计算预测值）
   ↓
4. 计算损失（损失函数）
   ↓
5. 反向传播（计算梯度）
   ↓
6. 参数更新（SGD）
   ↓
7. 重复步骤3-6直到收敛
```

### 完整实现

```python
# TODO: 待补充完整的神经网络训练代码实现
```

### 训练技巧总结

1. **数据预处理**：
   - 特征归一化/标准化
   - 数据增强
   - 打乱训练数据顺序

2. **参数初始化**：
   - Xavier初始化
   - He初始化
   - 避免全零初始化

3. **学习率设置**：
   - 从较大值开始尝试
   - 观察损失曲线调整
   - 使用学习率调度器

4. **防止过拟合**：
   - Dropout
   - L2正则化
   - Early Stopping
   - 数据增强

5. **监控训练过程**：
   - 记录训练/验证损失
   - 可视化学习曲线
   - 定期保存模型检查点

---

## 总结

本章介绍了神经网络学习的核心概念：

### 理论基础

1. **损失函数**：
   - 分类任务：交叉熵损失
   - 回归任务：MSE、MAE
   - 损失函数的选择影响训练效果

2. **数值微分**：
   - 导数和偏导数的数值计算
   - 梯度的概念和几何意义
   - 中心差分法提高精度

3. **梯度计算**：
   - 神经网络梯度的数值计算方法
   - 梯度检验确保实现正确性

4. **优化算法**：
   - 梯度下降法的原理
   - Epoch、Batch Size、Iteration的概念
   - SGD及其改进版本

### 关键收获

✅ 理解损失函数的作用和选择原则  
✅ 掌握数值微分和梯度计算方法  
✅ 理解梯度下降法的工作原理  
✅ 掌握训练相关的核心概念（Epoch、Batch、学习率）  
✅ 了解SGD优化器及其改进版本  

### 后续学习方向

1. **反向传播算法**：利用链式法则高效计算梯度
2. **高级优化器**：Adam、RMSprop等自适应优化器
3. **正则化技术**：防止过拟合的各种方法
4. **学习率调度**：动态调整学习率的策略
5. **分布式训练**：大规模数据的并行训练
