{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown_intro",
   "metadata": {},
   "source": [
    "# 循环神经网络(RNN)详解\n",
    "\n",
    "## 一、什么是RNN?\n",
    "\n",
    "循环神经网络(Recurrent Neural Network, RNN)是一种专门用于处理**序列数据**的神经网络。\n",
    "\n",
    "### RNN的核心特点\n",
    "\n",
    "- **记忆能力**:RNN具有记忆功能,能够保存之前的信息并影响当前的输出\n",
    "- **参数共享**:所有时间步共享相同的权重参数\n",
    "- **可变长度输入**:能够处理任意长度的序列数据\n",
    "- **时序依赖**:考虑数据之间的时间顺序关系\n",
    "\n",
    "### 为什么需要RNN?\n",
    "\n",
    "传统的前馈神经网络(如全连接网络、CNN)有一个局限:它们假设输入之间是独立的,无法处理具有时序关系的数据。而在很多实际场景中,数据是有时间顺序的:\n",
    "\n",
    "- **自然语言处理**:句子中词的顺序很重要,\"我喜欢你\"和\"你喜欢我\"意思完全不同\n",
    "- **时间序列预测**:股票价格、天气预报等,当前的值依赖于历史数据\n",
    "- **语音识别**:语音信号是连续的时序数据\n",
    "- **视频处理**:视频是连续的图像帧序列\n",
    "\n",
    "## 二、RNN的工作原理\n",
    "\n",
    "### 隐状态(Hidden State)\n",
    "\n",
    "RNN的核心概念是**隐状态**(Hidden State),它像是神经网络的\"记忆\":\n",
    "\n",
    "- 在每个时间步,RNN接收**当前输入**和**上一时刻的隐状态**\n",
    "- 计算得到**当前输出**和**当前隐状态**\n",
    "- 当前隐状态会传递到下一个时间步\n",
    "\n",
    "数学表达式:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} \\cdot h_t + b_y\n",
    "$$\n",
    "\n",
    "其中:\n",
    "- $h_t$: 当前时刻的隐状态\n",
    "- $h_{t-1}$: 上一时刻的隐状态(记忆)\n",
    "- $x_t$: 当前时刻的输入\n",
    "- $y_t$: 当前时刻的输出\n",
    "- $W_{hh}, W_{xh}, W_{hy}$: 权重矩阵(在所有时间步共享)\n",
    "- $b_h, b_y$: 偏置项\n",
    "\n",
    "### 多层RNN\n",
    "\n",
    "为了增强模型的表达能力,可以堆叠多个RNN层:\n",
    "- 第一层的输出作为第二层的输入\n",
    "- 每一层都有自己的隐状态\n",
    "- 可以学习不同层次的特征表示\n",
    "\n",
    "## 三、本示例的学习目标\n",
    "\n",
    "1. 理解PyTorch中`nn.RNN`的参数含义\n",
    "2. 掌握RNN的输入输出张量形状\n",
    "3. 理解隐状态的作用和维度\n",
    "4. 学习如何使用RNN处理序列数据\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-06T08:36:36.269346100Z",
     "start_time": "2026-02-06T08:36:36.242054800Z"
    }
   },
   "source": [
    "# ============================================\n",
    "# 导入必要的库\n",
    "# ============================================\n",
    "\n",
    "import torch              # PyTorch核心库,用于张量计算\n",
    "import torch.nn as nn     # PyTorch神经网络模块,提供RNN等层\n",
    "import torch.optim as optim  # PyTorch优化器模块,提供SGD、Adam等优化算法"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "markdown_rnn_layer",
   "metadata": {},
   "source": [
    "## 步骤1:创建RNN层\n",
    "\n",
    "### nn.RNN详解\n",
    "\n",
    "`nn.RNN`是PyTorch提供的标准RNN层实现。\n",
    "\n",
    "### 参数说明\n",
    "\n",
    "```python\n",
    "nn.RNN(input_size, hidden_size, num_layers, ...)\n",
    "```\n",
    "\n",
    "**核心参数:**\n",
    "\n",
    "1. **input_size=8**:输入特征的维度\n",
    "   - 表示每个时间步的输入向量有8个特征\n",
    "   - 例如:在词嵌入中,如果词向量维度是8,则input_size=8\n",
    "   - 在时间序列中,如果每个时刻有8个传感器读数,则input_size=8\n",
    "\n",
    "2. **hidden_size=16**:隐状态的维度\n",
    "   - 表示RNN内部的\"记忆\"用16维向量表示\n",
    "   - 也是输出向量的维度\n",
    "   - hidden_size越大,模型容量越大,但计算成本也越高\n",
    "   - 常见取值:64、128、256、512等\n",
    "\n",
    "3. **num_layers=2**:RNN的层数\n",
    "   - 表示堆叠2层RNN\n",
    "   - 第1层的输出作为第2层的输入\n",
    "   - 多层RNN能够学习更复杂的模式和更高层次的抽象\n",
    "   - 层数越多,表达能力越强,但也更容易过拟合\n",
    "\n",
    "**其他常用参数(本例未使用):**\n",
    "\n",
    "- `nonlinearity='tanh'`:激活函数,默认是tanh,也可以选择'relu'\n",
    "- `bias=True`:是否使用偏置项\n",
    "- `batch_first=False`:输入输出的维度顺序\n",
    "  - False(默认):(seq_len, batch, input_size)\n",
    "  - True:(batch, seq_len, input_size)\n",
    "- `dropout=0`:层间的dropout概率(当num_layers>1时有效)\n",
    "- `bidirectional=False`:是否使用双向RNN\n",
    "\n",
    "### RNN层的权重参数\n",
    "\n",
    "创建后,RNN层内部包含以下可学习的参数:\n",
    "- `weight_ih_l[k]`:第k层的输入到隐状态的权重,形状(hidden_size, input_size)\n",
    "- `weight_hh_l[k]`:第k层的隐状态到隐状态的权重,形状(hidden_size, hidden_size)\n",
    "- `bias_ih_l[k]`:第k层输入的偏置\n",
    "- `bias_hh_l[k]`:第k层隐状态的偏置\n",
    "\n",
    "这些参数在训练过程中会通过反向传播不断更新优化。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T08:36:36.283113500Z",
     "start_time": "2026-02-06T08:36:36.275472300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 步骤1:创建RNN层\n",
    "# ============================================\n",
    "\n",
    "# 创建一个两层的RNN网络\n",
    "# input_size=8:  每个时间步的输入特征维度为8\n",
    "# hidden_size=16: 隐状态的维度为16(也是输出维度)\n",
    "# num_layers=2:   堆叠2层RNN,增强模型表达能力\n",
    "\n",
    "rnn = nn.RNN(input_size=8, hidden_size=16, num_layers=2)\n",
    "\n",
    "# RNN的结构:\n",
    "# 输入(8维) -> RNN第1层(16维隐状态) -> RNN第2层(16维隐状态) -> 输出(16维)\n",
    "# 每一层都维护自己的隐状态,层与层之间隐状态独立"
   ],
   "id": "350a73d7fed834e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "markdown_input",
   "metadata": {},
   "source": [
    "## 步骤2:准备输入数据和初始隐状态\n",
    "\n",
    "### RNN的输入格式\n",
    "\n",
    "RNN需要两个输入:\n",
    "1. **输入序列**(input):待处理的序列数据\n",
    "2. **初始隐状态**(h0):RNN的初始记忆状态\n",
    "\n",
    "### 输入张量的形状\n",
    "\n",
    "#### input形状:(seq_len, batch, input_size)\n",
    "\n",
    "当`batch_first=False`(默认)时,输入形状为`(seq_len, batch, input_size)`:\n",
    "\n",
    "- **seq_len=3**:序列长度,表示有3个时间步\n",
    "  - 例如:一个句子有3个词\n",
    "  - 或者:一段时间序列有3个时刻的数据\n",
    "\n",
    "- **batch=3**:批次大小,表示同时处理3个样本\n",
    "  - 批处理可以加速训练\n",
    "  - 3个样本可以并行处理\n",
    "\n",
    "- **input_size=8**:每个时间步的特征维度\n",
    "  - 必须与RNN定义时的input_size一致\n",
    "  - 例如:每个词用8维向量表示\n",
    "\n",
    "**示例理解**:`input[0, 1, :]`表示:\n",
    "- 第1个时间步(第1个词)\n",
    "- 第2个样本(第2个句子)\n",
    "- 该词的8维特征向量\n",
    "\n",
    "#### h0形状:(num_layers, batch, hidden_size)\n",
    "\n",
    "初始隐状态的形状为`(num_layers, batch, hidden_size)`:\n",
    "\n",
    "- **num_layers=2**:RNN的层数\n",
    "  - 每一层都需要自己的初始隐状态\n",
    "  - hx[0]是第1层的初始隐状态\n",
    "  - hx[1]是第2层的初始隐状态\n",
    "\n",
    "- **batch=3**:批次大小,必须与input的batch一致\n",
    "  - 每个样本都有自己的隐状态\n",
    "\n",
    "- **hidden_size=16**:隐状态的维度\n",
    "  - 必须与RNN定义时的hidden_size一致\n",
    "\n",
    "### torch.randn()函数\n",
    "\n",
    "`torch.randn()`生成服从标准正态分布(均值0,标准差1)的随机张量:\n",
    "- 在实际应用中,初始隐状态通常初始化为全0:`torch.zeros(2, 3, 16)`\n",
    "- 这里使用随机初始化只是为了演示\n",
    "- 输入数据通常来自实际数据集,而不是随机生成"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T08:36:36.296973700Z",
     "start_time": "2026-02-06T08:36:36.284662600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 步骤2:准备输入数据和初始隐状态\n",
    "# ============================================\n",
    "\n",
    "# 1. 创建输入序列\n",
    "# 形状:(seq_len=3, batch=3, input_size=8)\n",
    "# - 序列长度为3:表示有3个时间步(例如句子有3个词)\n",
    "# - 批次大小为3:同时处理3个样本(例如3个句子)\n",
    "# - 特征维度为8:每个时间步的输入是8维向量\n",
    "# torch.randn()生成随机数据,实际应用中应该是真实的序列数据\n",
    "input = torch.randn(3, 3, 8)\n",
    "\n",
    "# 2. 创建初始隐状态\n",
    "# 形状:(num_layers=2, batch=3, hidden_size=16)\n",
    "# - 层数为2:因为RNN有2层,每层都需要初始隐状态\n",
    "# - 批次大小为3:必须与input的batch一致\n",
    "# - 隐状态维度为16:必须与RNN定义的hidden_size一致\n",
    "# 初始隐状态相当于RNN的\"初始记忆\",通常初始化为0或随机值\n",
    "hx = torch.randn(2, 3, 16)\n",
    "\n",
    "# 打印形状以验证\n",
    "print(input.shape)   # 输出: torch.Size([3, 3, 8])\n",
    "print(hx.shape)      # 输出: torch.Size([2, 3, 16])\n",
    "\n",
    "# 形状解释:\n",
    "# input[t, b, f] 表示: 第t个时间步,第b个样本,第f个特征\n",
    "# hx[l, b, h] 表示: 第l层,第b个样本,第h个隐状态维度"
   ],
   "id": "1080a9a75173c230",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 8])\n",
      "torch.Size([2, 3, 16])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "markdown_forward",
   "metadata": {},
   "source": [
    "## 步骤3:RNN前向传播\n",
    "\n",
    "### 前向传播过程\n",
    "\n",
    "调用RNN进行前向传播:\n",
    "```python\n",
    "output, hn = rnn(input, hx)\n",
    "```\n",
    "\n",
    "**输入:**\n",
    "- `input`:输入序列,形状(3, 3, 8)\n",
    "- `hx`:初始隐状态,形状(2, 3, 16)\n",
    "\n",
    "**输出:**\n",
    "- `output`:所有时间步的输出,形状(3, 3, 16)\n",
    "- `hn`:最后一个时间步的隐状态,形状(2, 3, 16)\n",
    "\n",
    "### 内部计算过程详解\n",
    "\n",
    "RNN会按照时间步顺序处理输入序列:\n",
    "\n",
    "**时间步1(t=0):**\n",
    "1. 取出input[0]作为第1个时间步的输入,形状(3, 8)\n",
    "2. 第1层RNN:\n",
    "   - 输入:input[0]和hx[0](第1层初始隐状态)\n",
    "   - 计算:h1_0 = tanh(W * input[0] + U * hx[0] + b)\n",
    "   - 输出:第1层的隐状态h1_0,形状(3, 16)\n",
    "3. 第2层RNN:\n",
    "   - 输入:h1_0(第1层的输出)和hx[1](第2层初始隐状态)\n",
    "   - 计算:h2_0 = tanh(W * h1_0 + U * hx[1] + b)\n",
    "   - 输出:第2层的隐状态h2_0,形状(3, 16)\n",
    "4. output[0] = h2_0(最后一层的输出)\n",
    "\n",
    "**时间步2(t=1):**\n",
    "1. 取出input[1]作为第2个时间步的输入\n",
    "2. 第1层:使用input[1]和上一步的h1_0,计算得到h1_1\n",
    "3. 第2层:使用h1_1和上一步的h2_0,计算得到h2_1\n",
    "4. output[1] = h2_1\n",
    "\n",
    "**时间步3(t=2):**\n",
    "1. 取出input[2]作为第3个时间步的输入\n",
    "2. 第1层:使用input[2]和h1_1,计算得到h1_2\n",
    "3. 第2层:使用h1_2和h2_1,计算得到h2_2\n",
    "4. output[2] = h2_2\n",
    "\n",
    "### 输出解释\n",
    "\n",
    "#### output形状:(seq_len, batch, hidden_size)\n",
    "\n",
    "`output`包含了**每个时间步最后一层RNN的输出**:\n",
    "\n",
    "- **形状:(3, 3, 16)**\n",
    "  - 3个时间步:output[0], output[1], output[2]\n",
    "  - 3个样本:每个时间步都有3个样本的输出\n",
    "  - 16维输出:等于hidden_size\n",
    "\n",
    "- **用途:**\n",
    "  - 序列标注任务:每个时间步都需要输出(如词性标注)\n",
    "  - 序列到序列:encoder-decoder架构\n",
    "  - 注意力机制:需要所有时间步的输出\n",
    "\n",
    "#### hn形状:(num_layers, batch, hidden_size)\n",
    "\n",
    "`hn`是**最后一个时间步每一层的隐状态**:\n",
    "\n",
    "- **形状:(2, 3, 16)**\n",
    "  - 2层:hn[0]是第1层最后的隐状态,hn[1]是第2层最后的隐状态\n",
    "  - 3个样本\n",
    "  - 16维隐状态\n",
    "\n",
    "- **特殊关系:**\n",
    "  - `hn[-1]`(最后一层的最后隐状态)等于`output[-1]`(最后时间步的输出)\n",
    "  - 即:hn[1] == output[2]\n",
    "\n",
    "- **用途:**\n",
    "  - 文本分类:只需要最后的隐状态表示整个序列\n",
    "  - 多层RNN堆叠:将hn传递给下一个RNN\n",
    "  - 序列生成:作为decoder的初始状态\n",
    "\n",
    "### 实际应用场景\n",
    "\n",
    "**1. 文本分类(只用hn):**\n",
    "```python\n",
    "output, hn = rnn(input, h0)\n",
    "final_hidden = hn[-1]  # 取最后一层的隐状态\n",
    "logits = classifier(final_hidden)  # 分类\n",
    "```\n",
    "\n",
    "**2. 序列标注(用output):**\n",
    "```python\n",
    "output, hn = rnn(input, h0)\n",
    "# output的每个时间步都进行预测\n",
    "predictions = classifier(output)  # 形状:(seq_len, batch, num_classes)\n",
    "```\n",
    "\n",
    "**3. Seq2Seq(用hn作为decoder初始状态):**\n",
    "```python\n",
    "# Encoder\n",
    "encoder_output, encoder_hn = encoder_rnn(source, h0)\n",
    "# Decoder使用encoder的最后隐状态作为初始状态\n",
    "decoder_output, decoder_hn = decoder_rnn(target, encoder_hn)\n",
    "```"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T08:36:36.313349800Z",
     "start_time": "2026-02-06T08:36:36.298497900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 步骤3:RNN前向传播\n",
    "# ============================================\n",
    "\n",
    "# 将输入序列和初始隐状态传入RNN\n",
    "# RNN会按时间步顺序处理输入,同时更新隐状态\n",
    "\n",
    "# 调用RNN进行前向传播\n",
    "# 输入: input(3,3,8) - 输入序列\n",
    "#      hx(2,3,16) - 初始隐状态\n",
    "# 输出: output - 每个时间步的输出\n",
    "#      hn - 最后一个时间步的隐状态\n",
    "output, hn = rnn(input, hx)\n",
    "\n",
    "# 打印输出形状\n",
    "print(output.shape)  # 输出: torch.Size([3, 3, 16])\n",
    "print(hn.shape)      # 输出: torch.Size([2, 3, 16])\n",
    "\n",
    "# ============================================\n",
    "# 形状解释\n",
    "# ============================================\n",
    "\n",
    "# output形状:(seq_len=3, batch=3, hidden_size=16)\n",
    "# - 包含所有3个时间步的输出\n",
    "# - output[0]:第1个时间步,所有样本的输出,形状(3,16)\n",
    "# - output[1]:第2个时间步,所有样本的输出,形状(3,16)\n",
    "# - output[2]:第3个时间步,所有样本的输出,形状(3,16)\n",
    "# - output实际上是最后一层RNN在每个时间步的隐状态\n",
    "\n",
    "# hn形状:(num_layers=2, batch=3, hidden_size=16)\n",
    "# - 包含最后一个时间步每一层的隐状态\n",
    "# - hn[0]:第1层RNN在最后时间步的隐状态,形状(3,16)\n",
    "# - hn[1]:第2层RNN在最后时间步的隐状态,形状(3,16)\n",
    "# - 注意:hn[1](第2层最后的隐状态)等于output[2](最后时间步的输出)\n",
    "\n",
    "# ============================================\n",
    "# 内部计算流程(简化理解)\n",
    "# ============================================\n",
    "\n",
    "# 时间步0: input[0] + hx[0] -> h1_0 (第1层)\n",
    "#                  h1_0 + hx[1] -> h2_0 (第2层) -> output[0]\n",
    "#\n",
    "# 时间步1: input[1] + h1_0 -> h1_1 (第1层)\n",
    "#                  h1_1 + h2_0 -> h2_1 (第2层) -> output[1]\n",
    "#\n",
    "# 时间步2: input[2] + h1_1 -> h1_2 (第1层) -> hn[0]\n",
    "#                  h1_2 + h2_1 -> h2_2 (第2层) -> output[2] = hn[1]\n",
    "\n",
    "# 可以验证: output[-1]和hn[-1]应该相等(最后一层的最后时间步)\n",
    "# print(torch.allclose(output[-1], hn[-1]))  # 输出: True"
   ],
   "id": "1ba7ccc02a6cf2bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 16])\n",
      "torch.Size([2, 3, 16])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "markdown_summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 总结:RNN的关键概念\n",
    "\n",
    "### 核心要点\n",
    "\n",
    "| 概念 | 说明 | 形状 |\n",
    "|------|------|------|\n",
    "| 输入序列(input) | 待处理的时序数据 | (seq_len, batch, input_size) |\n",
    "| 初始隐状态(h0) | RNN的初始记忆 | (num_layers, batch, hidden_size) |\n",
    "| 输出序列(output) | 每个时间步的输出 | (seq_len, batch, hidden_size) |\n",
    "| 最终隐状态(hn) | 最后时间步的隐状态 | (num_layers, batch, hidden_size) |\n",
    "\n",
    "### RNN的三个关键维度\n",
    "\n",
    "1. **时间维度(seq_len)**:序列有多长,有多少个时间步\n",
    "2. **批次维度(batch)**:同时处理多少个样本\n",
    "3. **特征维度(input_size/hidden_size)**:每个数据点的特征数\n",
    "\n",
    "### RNN的记忆机制\n",
    "\n",
    "```\n",
    "时间步0:  input[0] + h_0 -> h_1 -> output[0]\n",
    "                      ↓\n",
    "时间步1:  input[1] + h_1 -> h_2 -> output[1]\n",
    "                      ↓\n",
    "时间步2:  input[2] + h_2 -> h_3 -> output[2]\n",
    "```\n",
    "\n",
    "- 每个时间步的输出都依赖于当前输入和之前的隐状态\n",
    "- 隐状态像\"记忆\"一样,将历史信息传递到未来\n",
    "- 这使RNN能够捕捉序列中的时序依赖关系\n",
    "\n",
    "### RNN的优势与局限\n",
    "\n",
    "**优势:**\n",
    "- ✓ 能处理任意长度的序列\n",
    "- ✓ 参数共享,模型大小不随序列长度增长\n",
    "- ✓ 能捕捉时序依赖关系\n",
    "- ✓ 计算时考虑历史信息\n",
    "\n",
    "**局限:**\n",
    "- ✗ **梯度消失/爆炸**:难以学习长期依赖\n",
    "- ✗ **串行计算**:无法并行处理,训练慢\n",
    "- ✗ **记忆衰减**:时间步太长会遗忘早期信息\n",
    "\n",
    "### RNN的改进变体\n",
    "\n",
    "为了解决标准RNN的局限,研究者提出了多种改进:\n",
    "\n",
    "**1. LSTM(Long Short-Term Memory)**\n",
    "- 引入门控机制:输入门、遗忘门、输出门\n",
    "- 能够更好地捕捉长期依赖\n",
    "- PyTorch:`nn.LSTM()`\n",
    "\n",
    "**2. GRU(Gated Recurrent Unit)**\n",
    "- LSTM的简化版本,只有2个门\n",
    "- 参数更少,训练更快\n",
    "- PyTorch:`nn.GRU()`\n",
    "\n",
    "**3. 双向RNN(Bidirectional RNN)**\n",
    "- 同时从前向后和从后向前处理序列\n",
    "- 能同时利用过去和未来的信息\n",
    "- PyTorch:`nn.RNN(bidirectional=True)`\n",
    "\n",
    "### 实际应用建议\n",
    "\n",
    "1. **初学者**:先掌握基本RNN原理,理解时序建模思想\n",
    "2. **实际项目**:优先使用LSTM或GRU,性能更好\n",
    "3. **长序列**:考虑Transformer架构,解决了RNN的并行化问题\n",
    "4. **短序列**:RNN/LSTM/GRU都可以,选择最简单的\n",
    "5. **双向需求**:文本分类、序列标注等任务可用双向RNN\n",
    "\n",
    "### 下一步学习\n",
    "\n",
    "- 学习LSTM和GRU的门控机制\n",
    "- 了解Seq2Seq模型和注意力机制\n",
    "- 实践文本分类、情感分析等应用\n",
    "- 探索Transformer等现代序列模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
