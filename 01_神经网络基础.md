# 第 1 章 神经网络基础

## 1.1 神经网络的构成

### 1.1.1 基本概念和结构

神经网络是一种模仿生物神经系统的计算模型，由大量相互连接的神经元组成。基本结构包括：

- **输入层**：接收原始数据输入
- **隐藏层**：进行特征提取和转换
- **输出层**：产生最终预测结果

每个神经元包含：
- 权重（Weights）：控制输入信号的强度
- 偏置（Bias）：调整神经元的激活阈值
- 激活函数：引入非线性变换

### 1.1.2 复习感知机

感知机（Perceptron）是最简单的神经网络模型，由Rosenblatt在1957年提出。

**感知机模型**：
```
y = f(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)
```

其中：
- x₁, x₂, ..., xₙ 是输入特征
- w₁, w₂, ..., wₙ 是权重
- b 是偏置
- f 是激活函数（通常为阶跃函数）

**特点**：
- 只能解决线性可分问题
- 无法处理异或（XOR）等非线性问题
- 为多层神经网络奠定了基础

### 1.1.3 引入激活函数

激活函数的引入是神经网络能够处理复杂非线性问题的关键。

**为什么需要激活函数？**
- 引入非线性：使神经网络能够逼近任意复杂的函数
- 避免退化：没有激活函数的多层网络等价于单层线性模型
- 控制输出范围：将输出限制在特定区间内

## 1.2 激活函数

激活函数是神经网络中的核心组件，负责引入非线性变换。

### 1.2.1 激活函数的作用

1. **非线性映射**：将线性组合转换为非线性输出
2. **梯度传播**：在反向传播中传递梯度信息
3. **特征提取**：帮助网络学习数据的复杂模式
4. **输出归一化**：控制输出值的范围

### 1.2.2 阶跃（Binary step）函数

**定义**：
```
f(x) = { 1, if x ≥ 0
       { 0, if x < 0
```

**特点**：
- 输出只有0或1两个值
- 用于二分类问题
- 不可微，无法使用梯度下降

**缺点**：
- 梯度为0，无法进行反向传播
- 只能处理线性可分问题

### 1.2.3 Sigmoid函数

**定义**：
```
σ(x) = 1 / (1 + e⁻ˣ)
```

**特点**：
- 输出范围：(0, 1)
- 平滑可微
- 可以解释为概率

**优点**：
- 输出值在(0,1)之间，适合做概率预测
- 函数光滑，处处可导

**缺点**：
- 梯度消失问题：在输入值很大或很小时，梯度接近0
- 输出不以0为中心
- 计算开销较大（指数运算）

### 1.2.4 Tanh函数

**定义**：
```
tanh(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)
```

**特点**：
- 输出范围：(-1, 1)
- 以0为中心
- 是Sigmoid函数的变形

**优点**：
- 输出以0为中心，收敛速度比Sigmoid快
- 梯度比Sigmoid更强

**缺点**：
- 仍然存在梯度消失问题
- 计算开销较大

### 1.2.5 ReLU函数

**定义**：
```
ReLU(x) = max(0, x) = { x, if x > 0
                      { 0, if x ≤ 0
```

**特点**：
- 最常用的激活函数
- 计算简单高效
- 在正区间梯度恒为1

**优点**：
- 计算效率高，只需要一个阈值判断
- 缓解梯度消失问题（正区间）
- 收敛速度快
- 具有稀疏性，部分神经元输出为0

**缺点**：
- 神经元"死亡"问题：负区间梯度为0，可能导致神经元永久失活
- 输出不以0为中心

### 1.2.6 Softmax函数

**定义**：
```
softmax(xᵢ) = eˣⁱ / Σⱼ eˣʲ
```

**特点**：
- 用于多分类问题的输出层
- 将任意实数向量转换为概率分布
- 所有输出值之和为1

**应用场景**：
- 多分类问题的输出层
- 注意力机制
- 强化学习中的策略网络

**优点**：
- 输出可解释为概率
- 对所有类别进行归一化
- 突出最大值（概率最高的类别）

### 1.2.7 其他常见激活函数

1. **Leaky ReLU**：
   ```
   f(x) = { x,     if x > 0
          { αx,    if x ≤ 0  (α通常为0.01)
   ```
   - 解决ReLU的神经元死亡问题

2. **ELU (Exponential Linear Unit)**：
   ```
   f(x) = { x,              if x > 0
          { α(eˣ - 1),      if x ≤ 0
   ```
   - 输出均值接近0
   - 对噪声更鲁棒

3. **Swish**：
   ```
   f(x) = x · σ(x)
   ```
   - Google提出的自门控激活函数
   - 在深度网络中表现优异

### 1.2.8 如何选择激活函数

**选择指南**：

1. **隐藏层**：
   - 首选 **ReLU**：训练快，效果好
   - 考虑 **Leaky ReLU** 或 **ELU**：避免神经元死亡
   - 深度网络：尝试 **Swish**

2. **输出层**：
   - 二分类：**Sigmoid**
   - 多分类：**Softmax**
   - 回归问题：**线性激活**（无激活函数）或 **ReLU**

3. **特殊场景**：
   - RNN/LSTM：**Tanh** 和 **Sigmoid**
   - 生成模型：**Tanh**（输出需要在[-1,1]范围）

## 1.3 神经网络的简单实现

### 1.3.1 三层神经网络

三层神经网络是最基础的深度学习模型，包括：
- 输入层：接收原始数据
- 隐藏层：特征提取和转换
- 输出层：产生预测结果

**前向传播过程**：
```
1. 输入层 → 隐藏层：
   z₁ = W₁·x + b₁
   a₁ = f₁(z₁)

2. 隐藏层 → 输出层：
   z₂ = W₂·a₁ + b₂
   a₂ = f₂(z₂)
```

其中：
- W₁, W₂：权重矩阵
- b₁, b₂：偏置向量
- f₁, f₂：激活函数
- z：加权和
- a：激活输出

### 1.3.2 各层之间的信号传递

**信号传递机制**：

1. **前向传播（Forward Propagation）**：
   - 数据从输入层流向输出层
   - 每层进行线性变换和非线性激活
   - 最终产生预测结果

2. **反向传播（Backpropagation）**：
   - 计算损失函数对各参数的梯度
   - 梯度从输出层反向传播到输入层
   - 使用链式法则计算梯度

**矩阵运算**：
```
输入：X (n × m)  n个样本，m个特征
权重：W (m × k)  k个神经元
偏置：b (1 × k)
输出：Y (n × k)

Y = f(X·W + b)
```

### 1.3.3 代码实现

本节展示基于字典的简单实现方式，使用字典存储网络参数，适合快速原型开发和教学演示。

这是一个更轻量级的实现方式，使用字典存储网络参数，适合快速原型开发和教学演示。

```python
import numpy as np
from common.functions import sigmoid, identity

# 初始化网络参数
def init_network():
    """
    初始化三层神经网络的权重和偏置
    网络结构：
    - 输入层：2个神经元
    - 隐藏层1：3个神经元
    - 隐藏层2：2个神经元  
    - 输出层：2个神经元
    """
    network = {}
    
    # 第一层参数：输入层(2) → 隐藏层1(3)
    # W1的形状：(2, 3) - 2个输入特征，3个隐藏神经元
    network['W1'] = np.array([[0.1, 0.3, 0.5], 
                              [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])  # 形状：(3,)
    
    # 第二层参数：隐藏层1(3) → 隐藏层2(2)
    # W2的形状：(3, 2) - 3个输入，2个输出
    network['W2'] = np.array([[0.1, 0.4], 
                              [0.2, 0.5], 
                              [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])  # 形状：(2,)
    
    # 第三层参数：隐藏层2(2) → 输出层(2)
    # W3的形状：(2, 2)
    network['W3'] = np.array([[0.1, 0.3], 
                              [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])  # 形状：(2,)
    
    return network

# 前向传播
def forward(network, x):
    """
    执行前向传播计算
    
    参数:
        network: 包含网络参数的字典 (W1, W2, W3, b1, b2, b3)
        x: 输入数据，形状为 (n_features,) 或 (batch_size, n_features)
    
    返回:
        y: 网络的输出结果
    """
    # 提取网络参数
    w1, w2, w3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    # 第一层：输入层 → 隐藏层1
    # a1 = x·W1 + b1 （加权和）
    a1 = np.dot(x, w1) + b1
    # z1 = sigmoid(a1) （激活）
    z1 = sigmoid(a1)
    
    # 第二层：隐藏层1 → 隐藏层2
    # a2 = z1·W2 + b2 （加权和）
    a2 = np.dot(z1, w2) + b2
    # z2 = sigmoid(a2) （激活）
    z2 = sigmoid(a2)
    
    # 第三层（输出层）：隐藏层2 → 输出层
    # a3 = z2·W3 + b3 （加权和）
    a3 = np.dot(z2, w3) + b3
    # y = identity(a3) （恒等激活函数，用于回归问题）
    y = identity(a3)
    
    return y

# 测试流程
if __name__ == "__main__":
    # 初始化网络
    network = init_network()
    
    # 创建测试输入：2个特征
    x = np.array([1.0, 0.5])
    
    # 执行前向传播
    y = forward(network, x)
    
    # 输出结果
    print("输入:", x)
    print("输出:", y)
```

**代码说明**：

1. **网络初始化** (`init_network`)：
   - 使用字典存储所有参数，便于管理
   - 权重矩阵的形状由相邻层神经元数量决定
   - 这里使用固定值初始化，实际应用中通常使用随机初始化

2. **前向传播** (`forward`)：
   - 每一层的计算包括两步：线性变换（加权和）+ 非线性激活
   - 使用 `np.dot` 进行矩阵乘法，自动支持批量处理
   - 隐藏层使用 sigmoid 激活，输出层使用恒等函数

3. **信号流动过程**：
   ```
   输入 x (2,)
      ↓ [×W1 + b1]
   a1 (3,)
      ↓ [sigmoid]
   z1 (3,)
      ↓ [×W2 + b2]
   a2 (2,)
      ↓ [sigmoid]
   z2 (2,)
      ↓ [×W3 + b3]
   a3 (2,)
      ↓ [identity]
   输出 y (2,)
   ```

4. **参数形状分析**：
   - 输入层：2个特征
   - 第一隐藏层：3个神经元 → W1: (2,3), b1: (3,)
   - 第二隐藏层：2个神经元 → W2: (3,2), b2: (2,)
   - 输出层：2个神经元 → W3: (2,2), b3: (2,)

**说明**：

- 本实现使用字典存储网络参数，结构清晰
- 前向传播过程逐层计算，便于理解
- 激活函数来自 `common.functions` 模块
- 适合教学演示和快速验证想法

## 1.4 应用案例：手写数字识别

手写数字识别是深度学习的经典入门案例，通常使用MNIST数据集。

**问题描述**：
- 输入：28×28像素的灰度图像（784个特征）
- 输出：0-9的数字分类（10个类别）
- 训练集：60,000张图像
- 测试集：10,000张图像

**网络设计**：
```
输入层：784个神经元（28×28展平）
    ↓
隐藏层1：128个神经元，ReLU激活
    ↓
隐藏层2：64个神经元，ReLU激活（可选）
    ↓
输出层：10个神经元，Softmax激活
```

**实现步骤**：

1. **数据预处理**：
   - 归一化：将像素值从[0,255]缩放到[0,1]
   - 展平：将28×28矩阵展平为784维向量
   - 标签编码：转换为one-hot编码

2. **模型训练**：
   - 损失函数：交叉熵损失
   - 优化器：SGD、Adam等
   - 批量大小：32或64
   - 学习率：0.001-0.01

3. **模型评估**：
   - 准确率：预测正确的样本比例
   - 混淆矩阵：分析各类别的分类情况
   - 损失曲线：观察训练过程

### 实现方式一：单样本预测（基础版）

这是一个使用预训练模型进行手写数字识别的实例，不包含批量处理优化。

```python
import numpy as np
from common.functions import sigmoid, identity
import joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# ==================== 1. 数据加载和预处理 ====================
def get_data():
    """
    加载和预处理手写数字数据集
    
    返回:
        x_test: 测试集特征 (n_samples, 784)
        y_test: 测试集标签 (n_samples,)
    """
    # 从 CSV文件加载数据集
    # 数据格式：第一列为标签(0-9)，其余784列为像素值(0-255)
    data = pd.read_csv("../data/train.csv")
    
    # 划分特征和标签
    X = data.drop(["label"], axis=1)  # 删除标签列，保留784个特征
    y = data["label"]  # 提取标签列
    
    # 划分训练集和测试集（20%用于测试）
    x_train, x_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0
    )
    
    # 特征缩放：将像素值从[0, 255]缩放到[0, 1]
    # MinMaxScaler: X_scaled = (X - X_min) / (X_max - X_min)
    scaler = MinMaxScaler()
    x_train = scaler.fit_transform(x_train)  # 计算并应用缩放
    x_test = scaler.transform(x_test)  # 仅应用缩放（使用训练集的参数）
    
    return x_test, y_test

# ==================== 2. 网络初始化 ====================
def init_network():
    """
    从文件加载预训练的网络参数
    
    网络结构：
    - 输入层：784个神经元 (28x28 像素)
    - 隐藏层1：50个神经元
    - 隐藏层2：100个神经元
    - 输出层：10个神经元 (数字 0-9)
    
    返回:
        network: 包含权重和偏置的字典
    """
    # 使用 joblib 直接从文件加载字典对象
    # 文件包含: W1, W2, W3, b1, b2, b3
    network = joblib.load("../data/nn_sample")
    return network

# ==================== 3. 前向传播 ====================
def forward(network, x):
    """
    执行三层网络的前向传播
    
    参数:
        network: 网络参数字典
        x: 输入数据，形状为 (n_samples, 784) 或 (784,)
    
    返回:
        y: 输出概率分布，形状为 (n_samples, 10) 或 (10,)
    """
    # 提取网络参数
    w1, w2, w3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    # 第一层：输入层(784) → 隐藏层1(50)
    a1 = np.dot(x, w1) + b1  # 加权和
    z1 = sigmoid(a1)  # Sigmoid激活
    
    # 第二层：隐藏层1(50) → 隐藏层2(100)
    a2 = np.dot(z1, w2) + b2  # 加权和
    z2 = sigmoid(a2)  # Sigmoid激活
    
    # 第三层：隐藏层2(100) → 输出层(10)
    a3 = np.dot(z2, w3) + b3  # 加权和
    y = identity(a3)  # 恒等激活（返回原值）
    
    return y

# ==================== 4. 主流程 ====================
if __name__ == "__main__":
    # 加载数据
    x, y = get_data()
    print(f"测试集形状: {x.shape}")
    print(f"标签形状: {y.shape}")
    
    # 初始化网络
    network = init_network()
    
    # 打印网络结构信息（可选）
    print(f"\n网络结构:")
    print(f"W1 形状: {network['W1'].shape}  # 输入层 → 隐藏层1")
    print(f"W2 形状: {network['W2'].shape}  # 隐藏层1 → 隐藏层2")
    print(f"W3 形状: {network['W3'].shape}  # 隐藏层2 → 输出层")
    
    # 执行前向传播（整个测试集一次性预测）
    y_pred = forward(network, x)
    print(f"\n输出形状: {y_pred.shape}")
    
    # 将输出概率转换为预测标签
    # argmax 找到每行最大值的索引，即预测的数字
    y_pred = np.argmax(y_pred, axis=1)
    print(f"预测结果前10个: {y_pred[:10]}")
    print(f"真实标签前10个: {y.values[:10]}")
    
    # 计算分类准确率
    accuracy = np.mean(y_pred == y)
    print(f"\n准确率: {accuracy:.4f} ({accuracy*100:.2f}%)")
```

**代码说明**：

1. **数据预处理流程**：
   - 使用 pandas 读取 CSV 文件
   - 使用 sklearn 划分训练集和测试集
   - MinMaxScaler 将像素值归一化到 [0, 1] 区间

2. **模型加载**：
   - 使用 joblib 加载预训练模型
   - 模型包含所有训练好的权重和偏置

3. **预测流程**：
   - 前向传播计算得到原始输出
   - 使用 argmax 将概率分布转换为类别标签
   - 计算准确率：预测正确的样本比例

4. **性能问题**：
   - 此实现一次性处理所有测试数据
   - 当数据量大时可能导致内存不足
   - 下一个实现将解决这个问题

---

### 实现方式二：批量处理预测（优化版）

为了高效处理大规模数据，我们引入批量处理机制，将数据分批次送入网络进行预测。

```python
import numpy as np
from common.functions import sigmoid, identity
import joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# ==================== 1. 数据加载和预处理 ====================
def get_data():
    """
    加载和预处理手写数字数据集
    """
    data = pd.read_csv("../data/train.csv")
    X = data.drop(["label"], axis=1)
    y = data["label"]
    
    x_train, x_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=0
    )
    
    scaler = MinMaxScaler()
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)
    
    return x_test, y_test

# ==================== 2. 网络初始化 ====================
def init_network():
    """
    从文件加载预训练的网络参数
    """
    network = joblib.load("../data/nn_sample")
    return network

# ==================== 3. 前向传播 ====================
def forward(network, x):
    """
    执行三层网络的前向传播
    支持批量处理：x 可以是单个样本或一批样本
    """
    w1, w2, w3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    # 第一层：输入层 → 隐藏层1
    a1 = np.dot(x, w1) + b1
    z1 = sigmoid(a1)
    
    # 第二层：隐藏层1 → 隐藏层2
    a2 = np.dot(z1, w2) + b2
    z2 = sigmoid(a2)
    
    # 第三层：隐藏层2 → 输出层
    a3 = np.dot(z2, w3) + b3
    y = identity(a3)
    
    return y

# ==================== 4. 批量处理主流程 ====================
if __name__ == "__main__":
    # 加载数据
    x, y = get_data()
    print(f"测试集形状: {x.shape}")
    
    # 初始化网络
    network = init_network()
    
    # 设置批量参数
    batch_size = 100  # 每批处理100个样本
    accuracy_cnt = 0  # 统计预测正确的样本数
    n = x.shape[0]  # 总样本数
    
    print(f"\n开始批量预测...")
    print(f"总样本数: {n}")
    print(f"批次大小: {batch_size}")
    print(f"总批次数: {(n + batch_size - 1) // batch_size}")
    
    # 循环迭代，分批次处理
    for i in range(0, n, batch_size):
        # 提取当前批次的数据
        # 最后一批可能不足 batch_size 个样本
        x_batch = x[i:i+batch_size]
        
        # 对当前批次执行前向传播
        y_pred = forward(network, x_batch)
        
        # 将输出转换为预测标签
        p = np.argmax(y_pred, axis=1)
        
        # 统计当前批次预测正确的数量
        # np.sum(p == y[i:i+batch_size]) 计算匹配的数量
        accuracy_cnt += np.sum(p == y[i:i+batch_size])
        
        # 打印进度（每10批次）
        batch_num = i // batch_size + 1
        if batch_num % 10 == 0 or i + batch_size >= n:
            print(f"  处理批次 {batch_num}, 已处理样本: {min(i+batch_size, n)}/{n}")
    
    # 计算总体准确率
    accuracy = accuracy_cnt / n
    print(f"\n批量处理完成！")
    print(f"准确率: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"预测正确: {accuracy_cnt}/{n}")
```

**批量处理优势**：

1. **内存效率**：
   - 不需要一次性加载所有数据到内存
   - 适合处理大规模数据集
   - 降低内存峰值使用量

2. **计算效率**：
   - 利用 NumPy 的矩阵运算优化
   - 批量处理比单样本处理更快
   - GPU 加速时批量处理效果显著

3. **灵活性**：
   - 可以自由调整 batch_size
   - 适应不同的硬件环境
   - 支持进度显示

4. **批次大小选择指南**：
   - **小批量 (32-64)**：内存友好，但计算效率较低
   - **中等批量 (100-256)**：内存和速度平衡，推荐使用
   - **大批量 (512+)**：计算最快，但需要更多内存

**两种实现对比**：

| 特性 | 单样本预测 | 批量处理预测 |
|------|----------|------------|
| 内存使用 | 高（一次性加载全部） | 低（分批加载） |
| 计算速度 | 快（矩阵运算） | 快（矩阵运算） |
| 代码复杂度 | 简单 | 中等 |
| 适用场景 | 小数据集 | 大数据集 |
| 进度显示 | 无 | 有 |
| GPU优化 | 支持 | 更适合 |

**性能指标**：
- 简单三层网络：准确率约 95-97%
- 深度网络：准确率可达 98-99%
- CNN（卷积神经网络）：准确率可达 99%+

**关键要点**：
- 合适的网络深度和宽度
- 正确的激活函数选择
- 适当的正则化技术（Dropout、Batch Normalization）
- 充分的训练轮数
- 合适的学习率调整策略

**实战技巧总结**：

1. **数据预处理**：
   - 归一化处理（MinMaxScaler 或 StandardScaler）
   - 数据增强（针对图像数据）
   - 合理划分训练集和测试集

2. **模型优化**：
   - 使用批量处理提高效率
   - 选择合适的 batch_size（通常 32-256）
   - 监控训练过程，避免过拟合

3. **代码实践**：
   - 模块化设计：数据加载、网络定义、训练、评估分离
   - 使用字典或类来管理网络参数
   - 添加日志和进度显示
   - 保存训练好的模型（使用 joblib 或 torch.save）

4. **调试建议**：
   - 先用小数据集验证代码正确性
   - 检查数据形状（shape）是否匹配
   - 打印中间结果进行调试
   - 可视化训练曲线（损失和准确率）

---

## 总结

本章全面介绍了神经网络的基础知识，从理论到实践，内容包括：

### 理论基础

1. **神经网络的基本结构**：
   - 输入层、隐藏层、输出层的组成
   - 权重、偏置、激活函数的作用
   - 从感知机到多层网络的演化

2. **激活函数系统**：
   - 八种常用激活函数的详细介绍（Binary step、Sigmoid、Tanh、ReLU、Softmax 等）
   - 各激活函数的优缺点分析
   - 不同场景下的选择指南

3. **网络信号传递机制**：
   - 前向传播的计算过程
   - 层与层之间的矩阵运算
   - 激活函数的非线性变换

### 代码实现

1. **三层网络的基本实现**：
   - 基于字典的函数式实现，适合教学和快速原型
   - 完整的代码注释和参数形状分析
   - 激活函数来自 `common.functions` 模块

2. **手写数字识别实战案例**：
   - **实现一**：单样本预测（基础版）
     - 数据加载和预处理流程
     - 预训练模型的加载和使用
     - 完整的推理代码实现
   
   - **实现二**：批量处理预测（优化版）
     - 批量处理的实现原理
     - 内存优化和计算效率提升
     - 进度显示和统计分析

### 实践经验

1. **数据处理技巧**：
   - 特征归一化（MinMaxScaler）
   - 数据集划分策略
   - 批量大小选择

2. **模型设计要点**：
   - 网络结构设计（784-50-100-10）
   - 激活函数的合理选择
   - 参数初始化策略

3. **性能优化方法**：
   - 批量处理 vs 单样本处理
   - 内存使用优化
   - 计算效率提升

4. **代码工程化**：
   - 模块化设计原则
   - 参数管理方法
   - 调试和日志输出
   - 模型保存和加载

### 关键收获

通过本章学习，您应该掌握：

✅ 理解神经网络的基本原理和结构  
✅ 掌握各种激活函数的特点和应用场景  
✅ 能够从零实现一个简单的三层神经网络  
✅ 掌握批量处理的原理和实现方法  
✅ 能够使用神经网络解决实际问题（手写数字识别）  

### 后续学习路线

在掌握本章基础知识后，建议按以下路径深入学习：

1. **反向传播算法**：学习梯度计算和权重更新
2. **优化算法**：SGD、Adam、RMSprop 等
3. **正则化技术**：Dropout、Batch Normalization、L2 正则化
4. **卷积神经网络（CNN）**：处理图像数据的专用网络
5. **循环神经网络（RNN）**：处理序列数据的网络
6. **迁移学习**：使用预训练模型加速开发

这些基础知识为后续深入学习深度学习技术奠定了坚实的基础。通过结合理论学习和实际编码实践，您将能够更好地理解和应用神经网络技术。
