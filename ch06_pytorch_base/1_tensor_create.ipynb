#%%
# PyTorch张量(Tensor)创建方法学习笔记
# Tensor是PyTorch中的核心数据结构,类似于NumPy的ndarray,但支持GPU加速
# 张量可以是标量(0维)、向量(1维)、矩阵(2维)或更高维的多维数组
import torch
import numpy as np
#%% md
## 1. 基本张量创建
### (1) 按内容创建
#%%
# 创建标量(0维张量)
# torch.tensor()根据输入数据自动推断数据类型
# 输入整数→torch.int64, 输入浮点数→torch.float32
tensor1 = torch.tensor(10)
print("张量值:", tensor1)
print("张量形状:", tensor1.size())  # torch.Size([])表示0维标量
print("数据类型:", tensor1.dtype)   # torch.int64
#%%
# 从NumPy数组创建张量
# torch.tensor()会复制数据,修改tensor不影响原ndarray
# 对比: torch.from_numpy()共享内存,修改会互相影响
# 应用: 将NumPy数据转换为PyTorch张量用于神经网络训练
ndarray1 = np.array([[1, 2, 3], [4, 5, 6]])  # 2x3的NumPy数组
tensor2 = torch.tensor(ndarray1)
print("张量值:\n", tensor2)
print("张量形状:", tensor2.size())  # torch.Size([2, 3])
print("数据类型:", tensor2.dtype)   # torch.int64(继承NumPy的int类型)
#%% md
### (2) 创建指定形状的张量
#%%
# torch.Tensor(维度参数): 创建指定形状的未初始化张量
# 注意: 默认类型为float32, 值为内存中的随机数(不是真随机,是未初始化内存值)
# 应用: 占位符张量,后续会被赋值(如预先分配输出空间)
# 示例: torch.Tensor(3,2,4)创建3x2x4的三维张量,可理解为3个2x4矩阵
tensor1 = torch.Tensor(3, 2, 4)
print("张量值(未初始化,显示内存随机值):\n", tensor1)
print("张量形状:", tensor1.size())  # torch.Size([3, 2, 4])
print("数据类型:", tensor1.dtype)   # torch.float32(Tensor默认类型)
#%%
# torch.Tensor(列表/数组): 从数据创建张量并转换为float32
# 区别: torch.tensor()保持原数据类型, torch.Tensor()强制转为float32
# 示例: 输入整数列表[[1,2],[3,4]], Tensor()输出[[1.0,2.0],[3.0,4.0]]
tensor2 = torch.Tensor([[1, 2, 3, 4], [4, 5, 6, 7]])
print("张量值(整数被转为浮点):\n", tensor2)
print("张量形状:", tensor2.size())  # torch.Size([2, 4])
print("数据类型:", tensor2.dtype)   # torch.float32(被强制转换)
#%%
# torch.Tensor(单个整数): 创建指定长度的一维未初始化张量
# 注意: 与torch.tensor(10)不同! torch.tensor(10)创建标量,torch.Tensor(10)创建长度10的向量
# 对比: torch.tensor(10)→shape=[], torch.Tensor(10)→shape=[10]
tensor3 = torch.Tensor(10)  # 创建长度为10的一维张量
print("张量值(未初始化):", tensor3)
print("张量形状:", tensor3.size())  # torch.Size([10])
#%% md
### (3) 创建指定类型的张量
#%%
# 创建整数类型张量的三种方式
# torch.IntTensor: 32位整数(int32), 范围约±21亿
# torch.LongTensor: 64位整数(int64), 范围约±922亿亿, PyTorch默认整数类型
# 应用: 标签索引、数据索引通常用int64(可表示更大数据集)
tensor1 = torch.IntTensor([1, 2, 3])        # 方式1: 类型构造器→int32
tensor2 = torch.tensor([1, 2, 3], dtype=torch.int64)  # 方式2: dtype参数→int64
tensor3 = torch.LongTensor([1, 2, 3])      # 方式3: Long=int64别名
print("IntTensor类型:", tensor1.dtype)     # torch.int32
print("dtype=int64类型:", tensor2.dtype)   # torch.int64
print("LongTensor类型:", tensor3.dtype)    # torch.int64
#%%
# 创建短整数类型张量(int16)
# torch.short/ShortTensor: 16位整数, 范围-32768到32767
# 应用: 节省内存的小范围整数存储(如图像像素索引、类别数<32767的标签)
# 对比: int16(2字节) < int32(4字节) < int64(8字节), 但表示范围递减
tensor1 = torch.ShortTensor([1, 2, 3])
print("ShortTensor类型:", tensor1.dtype)   # torch.int16
tensor2 = torch.tensor([1, 2, 3], dtype=torch.short)
print("dtype=short类型:", tensor2.dtype)   # torch.int16
#%%
# 创建字节类型张量
# torch.ByteTensor: 8位无符号整数(uint8), 范围0-255
# torch.int8: 8位有符号整数, 范围-128到127
# 应用: 图像数据(像素值0-255)、掩码(0/1二值)、极小范围整数存储
tensor1 = torch.ByteTensor([1, 2, 3])      # 无符号uint8
print("ByteTensor类型:", tensor1.dtype)    # torch.uint8
tensor2 = torch.tensor([1, 2, 3], dtype=torch.int8)  # 有符号int8
print("dtype=int8类型:", tensor2.dtype)    # torch.int8
#%%
# 创建单精度浮点数张量(float32)
# torch.float32/FloatTensor: PyTorch默认浮点类型, 32位单精度, 精度约7位小数
# 应用: 神经网络权重、激活值、损失值等绝大多数深度学习计算(速度与精度平衡)
# 示例: 0.1234567→float32存储为0.1234567, float16可能变为0.1235
tensor1 = torch.FloatTensor([1, 2, 3])
print("FloatTensor类型:", tensor1.dtype)   # torch.float32
tensor2 = torch.tensor([1, 2, 3], dtype=torch.float32)
print("dtype=float32类型:", tensor2.dtype) # torch.float32
#%%
# 创建双精度浮点数张量(float64)
# torch.float64/DoubleTensor: 64位双精度, 精度约15位小数, 内存是float32的2倍
# 应用: 科学计算、高精度数值模拟(深度学习中较少用,因速度慢且显存占用大)
# 对比: float32精度够用且快, float64精度高但慢, float16快但精度低
tensor1 = torch.DoubleTensor(2, 3)  # 未初始化的2x3双精度张量
tensor2 = torch.tensor([1, 2, 3], dtype=torch.float64)
print("DoubleTensor类型:", tensor1.dtype)  # torch.float64
print("dtype=float64类型:", tensor2.dtype) # torch.float64
print("tensor1值(未初始化):", tensor1)
#%%
# 创建半精度浮点数张量(float16)
# torch.float16/HalfTensor: 16位半精度, 精度约3位小数, 内存是float32的1/2
# 应用: 混合精度训练(AMP)、GPU显存受限时、推理加速(精度损失可接受场景)
# 优势: 显存占用少、GPU计算快(Tensor Core加速) | 劣势: 数值范围小易溢出
tensor1 = torch.tensor([1, 2, 3], dtype=torch.float16)
print("dtype=float16类型:", tensor1.dtype) # torch.float16
tensor2 = torch.tensor([1, 2, 3], dtype=torch.half)  # half是float16别名
print("dtype=half类型:", tensor2.dtype)    # torch.float16
#%%
# 创建布尔类型张量
# torch.bool: 存储True/False, 仅占1位(内存高效)
# 应用: 掩码(mask)、条件筛选、注意力机制的padding标记
# 示例: mask=[True,False,True]筛选data=[1,2,3]→结果[1,3]
tensor1 = torch.BoolTensor([True, False, True])
print("BoolTensor类型:", tensor1.dtype)    # torch.bool
tensor2 = torch.tensor([True, False, True], dtype=torch.bool)
print("dtype=bool类型:", tensor2.dtype)    # torch.bool
#%% md
## 2. 指定区间的张量创建
#%%
# torch.arange(start, end, step): 生成等差数列
# 参数: start(起始,包含), end(终止,不包含), step(步长)
# 类似Python的range(), 但返回张量而非列表
# 示例: arange(10,30,2)→[10,12,14,...,28] (不含30)
tensor1 = torch.arange(10, 30, 2)  # 从10到30(不含), 步长2
print("等差数列:", tensor1)  # tensor([10, 12, 14, 16, 18, 20, 22, 24, 26, 28])
#%%
# torch.arange(n): 生成0到n-1的整数序列
# 等价于arange(0, n, 1), 常用于生成索引
# 应用: 批次索引、数据采样索引、位置编码
tensor1 = torch.arange(6)  # 生成[0,1,2,3,4,5]
print("索引序列:", tensor1)
#%%
# torch.linspace(start, end, steps): 在区间均匀生成指定数量的点
# 参数: start(起始,包含), end(终止,包含), steps(生成点数)
# 区别arange: linspace指定点数, arange指定步长
# 示例: linspace(10,30,5)在[10,30]间均匀取5个点→[10,15,20,25,30]
# 应用: 生成学习率衰减序列、可视化采样点
tensor1 = torch.linspace(10, 30, 5)  # 在[10,30]均匀取5个点
print("均匀采样点:", tensor1)  # tensor([10., 15., 20., 25., 30.])
#%%
# torch.logspace(start, end, steps, base): 在对数空间均匀生成点
# 参数: 生成base^start到base^end的steps个点
# 公式: [base^start, base^(start+Δ), ..., base^end], 其中Δ=(end-start)/(steps-1)
# 示例: logspace(1,3,3,base=2)→[2^1, 2^2, 2^3]=[2,4,8]
# 应用: 学习率指数衰减、对数刻度的超参数搜索
tensor1 = torch.logspace(1, 3, 3, 2)  # base=2, 从2^1到2^3取3个点
print("对数空间点:", tensor1)  # tensor([2., 4., 8.])
#%% md
## 3. 按数值填充张量
#%%
# torch.zeros(*shape): 创建全0张量
# 应用: 偏置初始化为0、梯度清零、占位符张量
# 示例: zeros(2,3)→[[0,0,0],[0,0,0]] (2行3列全0矩阵)
tensor1 = torch.zeros(2, 3)
print("全0张量:\n", tensor1)
#%%
# torch.ones(*shape): 创建全1张量
# 应用: 权重初始化、掩码初始化
# 示例: ones(2,3)→[[1,1,1],[1,1,1]] (2行3列全1矩阵)
tensor1 = torch.ones(2, 3)
print("全1张量:\n", tensor1)
#%%
# torch.full(shape, value): 创建全部填充指定值的张量
# 应用: 初始化LSTM遗忘门偏置为1、常量掩码、特定值填充
# 示例: full((3,2), 6)→[[6,6],[6,6],[6,6]] (3x2全6矩阵)
tensor1 = torch.full((3, 2), 6)  # 注意:shape用元组
print("全6张量:\n", tensor1)
#%%
# torch.eye(n): 创建nxn单位矩阵(对角线为1,其余为0)
# 应用: 矩阵初等变换、残差连接初始化、协方差矩阵初始化
# 示例: eye(3)→[[1,0,0],[0,1,0],[0,0,1]] (3x3单位矩阵)
tensor1 = torch.eye(3)
print("单位矩阵:\n", tensor1)
#%%
# torch.zeros_like(input): 创建与input相同形状的全0张量
# 应用: 梯度初始化、创建相同形状的辅助张量
tensor2 = torch.zeros_like(tensor1)  # 与tensor1同形状(3x3)
print("zeros_like张量:\n", tensor2)
#%%
# torch.empty_like(input): 创建与input相同形状和数据类型的未初始化张量
# 区别zeros_like: empty_like不初始化(内存随机值), 创建速度更快
# 应用: 预分配输出空间(后续会完全覆盖,不需要初始化为0)
# 注意: 值不可预测,使用前必须赋值,否则结果错误
tensor3 = torch.empty_like(tensor1)  # 与tensor1同形状(3x3)
print("未初始化张量(随机值):\n", tensor3)
#%% md
## 4. 随机张量创建
#%%
# (1) rand: 生成[0, 1)区间均匀分布的随机数
# 用途:权重初始化、数据增强、dropout等需要均匀随机值的场景
# 示例: torch.rand(2, 3)生成2x3张量,元素如[[0.234, 0.891, 0.456], [0.123, 0.678, 0.901]]
tensor1 = torch.rand(2, 3)
print("均匀分布随机数:\n", tensor1)
print("数据类型:", tensor1.dtype)  # torch.float32
#%%
# (2) randn: 生成标准正态分布(均值0,方差1)的随机数
# 用途:神经网络权重初始化(Xavier/He初始化的基础)、噪声生成
# 示例: torch.randn(2, 3)生成2x3张量,元素如[[0.34, -1.23, 0.56], [-0.89, 1.45, -0.12]]
tensor2 = torch.randn(3, 4)
print("正态分布随机数:\n", tensor2)
print("数据类型:", tensor2.dtype)  # torch.float32
#%%
# (3) randint: 生成指定范围[low, high)的整数随机数
# 用途:生成标签索引、随机采样、数据打乱等需要整数随机值的场景
# 示例: torch.randint(0, 10, (2, 3))生成[0,10)的整数,如[[3, 7, 1], [9, 0, 5]]
tensor3 = torch.randint(0, 10, (3, 3))
print("整数随机数:\n", tensor3)
print("数据类型:", tensor3.dtype)  # torch.int64
#%%
# (4) randperm: 生成0到n-1的随机排列
# 用途:数据打乱顺序、随机采样索引、交叉验证数据分割
# 示例: torch.randperm(10)生成[0,1,2,...,9]的随机排列,如[3, 7, 1, 9, 0, 5, 2, 8, 4, 6]
tensor4 = torch.randperm(10)
print("随机排列:", tensor4)
print("数据类型:", tensor4.dtype)  # torch.int64
#%%
# (5) normal: 生成指定均值和标准差的正态分布随机数
# 用途:自定义权重初始化(如LSTM的遗忘门偏置初始化为均值1)、生成特定分布的噪声
# 示例: torch.normal(mean=5.0, std=2.0, size=(2,3))生成均值5,标准差2的张量
#       结果如[[5.67, 3.21, 6.89], [4.12, 7.45, 3.78]]
tensor5 = torch.normal(mean=0.0, std=1.0, size=(2, 4))
print("自定义正态分布:\n", tensor5)
print("数据类型:", tensor5.dtype)  # torch.float32
#%%
# (6) rand_like/randn_like: 生成与给定张量相同形状的随机张量
# 用途:保持张量形状一致的随机初始化、生成相同形状的噪声
# 示例: 若x.shape=(3,4), torch.rand_like(x)生成3x4的[0,1)均匀分布张量
x = torch.zeros(2, 3)
tensor6 = torch.rand_like(x)  # 生成与x相同形状的[0,1)均匀分布张量
print("rand_like结果:\n", tensor6)
tensor7 = torch.randn_like(x)  # 生成与x相同形状的标准正态分布张量
print("randn_like结果:\n", tensor7)
#%%
# (7) 设置随机种子:保证随机数可复现
# 用途:实验结果可重复、调试代码、对比不同模型在相同随机初始化下的表现
# 示例: 设置seed=42后,每次运行torch.randn(2,2)都会得到相同的结果
torch.manual_seed(42)
tensor8 = torch.randn(2, 2)
print("第一次生成:\n", tensor8)

torch.manual_seed(42)  # 重新设置相同种子
tensor9 = torch.randn(2, 2)
print("第二次生成(相同种子):\n", tensor9)
print("两次结果是否相等:", torch.equal(tensor8, tensor9))