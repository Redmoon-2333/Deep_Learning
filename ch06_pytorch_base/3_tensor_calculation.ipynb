{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.250865400Z",
     "start_time": "2026-02-01T15:55:34.653462200Z"
    }
   },
   "source": [
    "# PyTorch张量计算操作学习笔记\n",
    "# 涵盖:四则运算、矩阵运算、原地操作、内存管理\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.263878200Z",
     "start_time": "2026-02-01T15:55:36.251865300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 四则运算\n",
    "# PyTorch支持逐元素运算和广播机制\n",
    "# 广播:当两个张量形状不同时,自动扩展维度进行运算\n",
    "# 示例:shape(2,3)+shape(3,)会将后者扩展为(1,3)再广播为(2,3)\n",
    "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "# 张量间加法(逐元素相加)\n",
    "print(a + b)  # [[1+5, 2+6], [3+7, 4+8]] = [[6,8], [10,12]]\n",
    "\n",
    "# 张量与标量运算(广播机制)\n",
    "print(a+10)  # 每个元素+10: [[11,12], [13,14]]\n",
    "\n",
    "# .add()方法:返回新张量,不修改原张量\n",
    "print(a.add(10))  # 结果[[11,12], [13,14]], 但a仍是[[1,2], [3,4]]\n",
    "\n",
    "# .add_()原地操作:带下划线后缀的方法会直接修改原张量,节省内存\n",
    "# 应用:梯度累积、参数更新等需要原地修改的场景\n",
    "print(a.add_(10))  # a被修改为[[11,12], [13,14]]\n",
    "print(a)  # 验证a已被修改"
   ],
   "id": "844c117dd369bf90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[11., 12.],\n",
      "        [13., 14.]])\n",
      "tensor([[11., 12.],\n",
      "        [13., 14.]])\n",
      "tensor([[11., 12.],\n",
      "        [13., 14.]])\n",
      "tensor([[11., 12.],\n",
      "        [13., 14.]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.279061600Z",
     "start_time": "2026-02-01T15:55:36.270817600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 减法运算\n",
    "# .sub_()原地减法,恢复a的原始值\n",
    "# 对应:.sub()不修改原张量, .sub_()修改原张量\n",
    "print(a.sub_(10))  # a从[[11,12],[13,14]]变回[[1,2],[3,4]]"
   ],
   "id": "385e4c02c1eb3dd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.294923700Z",
     "start_time": "2026-02-01T15:55:36.286068300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 幂运算\n",
    "# .pow_(2)计算每个元素的平方\n",
    "# 应用:计算L2范数、方差等统计量\n",
    "# 示例:[[1,2],[3,4]]^2 = [[1,4],[9,16]]\n",
    "print(a.pow_(2))  # 原地平方运算"
   ],
   "id": "b44aea752790379c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.316108100Z",
     "start_time": "2026-02-01T15:55:36.300926200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 开方运算\n",
    "# .sqrt_()计算每个元素的平方根\n",
    "# 这里将[[1,4],[9,16]]开方恢复为[[1,2],[3,4]]\n",
    "# 应用:标准差计算、向量归一化\n",
    "print(a.sqrt_())  # 原地开方运算"
   ],
   "id": "6be8cb9361d70021",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.331132500Z",
     "start_time": "2026-02-01T15:55:36.317110400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 指数运算(Python运算符)\n",
    "# 使用**运算符计算e^x, 其中e≈2.7183(自然对数底数)\n",
    "# 示例:e^[1,2,3] ≈ [2.72, 7.39, 20.09]\n",
    "# 应用:激活函数(如softmax中的exp运算)\n",
    "tensor1=torch.tensor([1.0,2,3])\n",
    "print(2.7183**tensor1)  # [e^1, e^2, e^3]"
   ],
   "id": "ea95c0750fb3095f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7183,  7.3892, 20.0859])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.347125100Z",
     "start_time": "2026-02-01T15:55:36.332132800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 指数运算(PyTorch方法)\n",
    "# .exp()使用真实的e值计算,结果更精确\n",
    "# 对比上一cell:这里用math.e,上面用2.7183近似值\n",
    "# 应用:softmax、log-sum-exp技巧、指数族分布\n",
    "print(tensor1.exp())  # [e^1, e^2, e^3]精确计算"
   ],
   "id": "b777de36cbd73a5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7183,  7.3891, 20.0855])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.363463700Z",
     "start_time": "2026-02-01T15:55:36.347627100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 哈达玛积(Hadamard Product):逐元素相乘\n",
    "# 区别矩阵乘法:对应位置元素相乘,不是线性代数的矩阵乘法\n",
    "# 示例:[[1,2],[3,4]] ⊙ [[5,6],[7,8]] = [[1*5,2*6],[3*7,4*8]] = [[5,12],[21,32]]\n",
    "# 应用:注意力机制的mask、dropout、特征融合\n",
    "tensor1 = torch.tensor([[1,2],[3,4]])\n",
    "tensor2 = torch.tensor([[5,6],[7,8]])\n",
    "print(tensor1 * tensor2)  # 运算符方式\n",
    "print(torch.mul(tensor1,tensor2))  # 函数方式(等价)"
   ],
   "id": "414d3e652dc1e38c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.389779400Z",
     "start_time": "2026-02-01T15:55:36.363965900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 矩阵乘法(Matrix Multiplication):线性代数中的矩阵乘法\n",
    "# 规则:(m,n)@(n,p)→(m,p), 第一个的列数必须等于第二个的行数\n",
    "# 计算:结果[i,j] = Σ(A[i,k] * B[k,j])\n",
    "# 示例:[[1,2],[3,4]] @ [[5,6],[7,8]]\n",
    "#       →[[1*5+2*7, 1*6+2*8],[3*5+4*7, 3*6+4*8]] = [[19,22],[43,50]]\n",
    "# 应用:全连接层、注意力分数计算、线性变换\n",
    "tensor1 = torch.tensor([[1,2],[3,4]])\n",
    "tensor2 = torch.tensor([[5,6],[7,8]])\n",
    "print(tensor1 @ tensor2)  # @运算符(推荐)\n",
    "print(torch.matmul(tensor1,tensor2))  # matmul函数(等价)\n",
    "\n",
    "# 批量矩阵乘法(Batched Matrix Multiplication)\n",
    "# 三维张量:(batch, m, n) @ (batch, n, p) → (batch, m, p)\n",
    "# 每个batch独立进行矩阵乘法\n",
    "# 应用:Transformer中的批量注意力计算\n",
    "tensor1 = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])  # shape:(2,2,2)\n",
    "tensor2 = torch.tensor([[[9,10],[11,12]],[[13,14],[15,16]]])  # shape:(2,2,2)\n",
    "print(tensor1 @ tensor2)  # batch0:[[1,2],[3,4]]@[[9,10],[11,12]]\n",
    "print(torch.matmul(tensor1,tensor2))  # batch1:[[5,6],[7,8]]@[[13,14],[15,16]]"
   ],
   "id": "6395c858a677e160",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "tensor([[[ 31,  34],\n",
      "         [ 71,  78]],\n",
      "\n",
      "        [[155, 166],\n",
      "         [211, 226]]])\n",
      "tensor([[[ 31,  34],\n",
      "         [ 71,  78]],\n",
      "\n",
      "        [[155, 166],\n",
      "         [211, 226]]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.415317300Z",
     "start_time": "2026-02-01T15:55:36.389779400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 内存分配验证\n",
    "# Python的id()返回对象的内存地址\n",
    "# 非原地操作(如X=X+10)会创建新张量,内存地址改变\n",
    "# 问题:大张量频繁运算会导致内存碎片和性能下降\n",
    "X = torch.randint(1,10,(3,2,4))\n",
    "print(id(X))  # 记录原始内存地址\n",
    "X=X+10  # 创建新张量并重新赋值给X\n",
    "print(id(X))  # 内存地址已改变(新对象)"
   ],
   "id": "2f1f7f515903b11f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2526053193344\n",
      "2526053197824\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T15:55:36.435840500Z",
     "start_time": "2026-02-01T15:55:36.416819500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 原地操作节省内存\n",
    "# 方法1: X = X @ Y (创建新张量,内存地址变)\n",
    "# 方法2: X += 10 (语法糖,等价于X = X + 10,也创建新张量)\n",
    "# 方法3: X[:] = X @ Y (切片赋值,原地修改,内存地址不变)\n",
    "# 应用:训练大模型时减少显存占用\n",
    "X = torch.randint(1, 10, (3, 2, 4))\n",
    "Y = torch.randint(1, 10, (3, 4, 1))\n",
    "print(X)\n",
    "print(X.shape)  # (3,2,4)\n",
    "print(id(X))  # 原始地址\n",
    "print(X @ Y)  # (3,2,4)@(3,4,1)→(3,2,1)\n",
    "\n",
    "# 注意:X[:]=会自动广播结果(3,2,1)→(3,2,4)以匹配X的形状\n",
    "# 这会导致数据重复填充(见输出中每行4个元素相同)\n",
    "#X = X @ Y  # 这会改变地址\n",
    "# X += 10  # 这也会改变地址\n",
    "X[:]= X @ Y  # 切片赋值,保持原地址\n",
    "print(X)\n",
    "print(X.shape)  # 仍是(3,2,4)\n",
    "print(id(X))  # 地址未变,证明是原地操作"
   ],
   "id": "38cd9c871eaccdf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[7, 6, 3, 6],\n",
      "         [9, 5, 7, 8]],\n",
      "\n",
      "        [[4, 7, 7, 7],\n",
      "         [7, 8, 8, 5]],\n",
      "\n",
      "        [[6, 2, 3, 7],\n",
      "         [4, 1, 5, 5]]])\n",
      "torch.Size([3, 2, 4])\n",
      "2526053197264\n",
      "tensor([[[129],\n",
      "         [167]],\n",
      "\n",
      "        [[137],\n",
      "         [135]],\n",
      "\n",
      "        [[111],\n",
      "         [ 78]]])\n",
      "tensor([[[129, 129, 129, 129],\n",
      "         [167, 167, 167, 167]],\n",
      "\n",
      "        [[137, 137, 137, 137],\n",
      "         [135, 135, 135, 135]],\n",
      "\n",
      "        [[111, 111, 111, 111],\n",
      "         [ 78,  78,  78,  78]]])\n",
      "torch.Size([3, 2, 4])\n",
      "2526053197264\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
