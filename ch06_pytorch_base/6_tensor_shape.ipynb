{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.415224500Z",
     "start_time": "2026-02-01T17:12:57.851222800Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch张量形状操作学习笔记\n",
    "# 形状操作是深度学习中非常重要的技能,用于调整数据维度以匹配模型输入输出\n",
    "# 包括:维度交换、重塑、维度增减、张量拼接等操作\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. 查看张量形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "check_shape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.437063900Z",
     "start_time": "2026-02-01T17:12:59.416227600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量形状: torch.Size([2, 3, 6])\n",
      "张量维度数: 3\n",
      "张量元素总数: 36\n",
      "张量内容:\n",
      " tensor([[[3, 2, 9, 1, 1, 8],\n",
      "         [3, 2, 9, 3, 9, 6],\n",
      "         [7, 8, 2, 9, 7, 1]],\n",
      "\n",
      "        [[2, 5, 4, 5, 3, 4],\n",
      "         [7, 7, 8, 3, 8, 5],\n",
      "         [7, 7, 8, 6, 1, 2]]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个三维张量用于演示\n",
    "# shape=[2,3,6]可理解为: 2个样本,每个样本3行6列的矩阵\n",
    "# 应用场景: 批次数据(batch_size=2, height=3, width=6)\n",
    "tensor1 = torch.randint(1, 10, [2, 3, 6])\n",
    "print(\"张量形状:\", tensor1.shape)      # torch.Size([2, 3, 6])\n",
    "print(\"张量维度数:\", tensor1.ndim)     # 3 (三维张量)\n",
    "print(\"张量元素总数:\", tensor1.numel()) # 2*3*6=36\n",
    "print(\"张量内容:\\n\", tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. 维度交换操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "transpose",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.457132600Z",
     "start_time": "2026-02-01T17:12:59.440076400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([2, 3, 6])\n",
      "交换后形状: torch.Size([2, 6, 3])\n",
      "交换后内容:\n",
      " tensor([[[3, 3, 7],\n",
      "         [2, 2, 8],\n",
      "         [9, 9, 2],\n",
      "         [1, 3, 9],\n",
      "         [1, 9, 7],\n",
      "         [8, 6, 1]],\n",
      "\n",
      "        [[2, 7, 7],\n",
      "         [5, 7, 7],\n",
      "         [4, 8, 8],\n",
      "         [5, 3, 6],\n",
      "         [3, 8, 1],\n",
      "         [4, 5, 2]]])\n"
     ]
    }
   ],
   "source": [
    "# transpose(dim0, dim1): 交换两个指定维度\n",
    "# 应用: 矩阵转置、调整通道顺序(如NCHW↔NHWC)\n",
    "# 示例: shape=[2,3,6]交换维度1和2→shape=[2,6,3]\n",
    "# 注意: transpose只能交换两个维度,多维交换需用permute\n",
    "tensor2 = tensor1.transpose(1, 2)  # 交换第1维(3)和第2维(6)\n",
    "print(\"原始形状:\", tensor1.shape)    # torch.Size([2, 3, 6])\n",
    "print(\"交换后形状:\", tensor2.shape)  # torch.Size([2, 6, 3])\n",
    "print(\"交换后内容:\\n\", tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "transpose_t",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.484929700Z",
     "start_time": "2026-02-01T17:12:59.462136100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始矩阵形状: torch.Size([3, 4])\n",
      "转置后形状: torch.Size([4, 3])\n",
      "原始矩阵:\n",
      " tensor([[2, 1, 5, 8],\n",
      "        [1, 1, 7, 3],\n",
      "        [4, 8, 1, 3]])\n",
      "转置矩阵:\n",
      " tensor([[2, 1, 4],\n",
      "        [1, 1, 8],\n",
      "        [5, 7, 1],\n",
      "        [8, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "# .T: 二维张量的转置快捷方式(等价于transpose(0,1))\n",
    "# 应用: 矩阵转置、线性代数运算\n",
    "# 注意: .T只适用于二维张量,高维张量会交换前两维\n",
    "matrix = torch.randint(1, 10, [3, 4])  # 3x4矩阵\n",
    "print(\"原始矩阵形状:\", matrix.shape)    # torch.Size([3, 4])\n",
    "print(\"转置后形状:\", matrix.T.shape)    # torch.Size([4, 3])\n",
    "print(\"原始矩阵:\\n\", matrix)\n",
    "print(\"转置矩阵:\\n\", matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "permute",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.517279100Z",
     "start_time": "2026-02-01T17:12:59.486932600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([2, 3, 6])\n",
      "permute后形状: torch.Size([6, 2, 3])\n",
      "permute后内容:\n",
      " tensor([[[3, 3, 7],\n",
      "         [2, 7, 7]],\n",
      "\n",
      "        [[2, 2, 8],\n",
      "         [5, 7, 7]],\n",
      "\n",
      "        [[9, 9, 2],\n",
      "         [4, 8, 8]],\n",
      "\n",
      "        [[1, 3, 9],\n",
      "         [5, 3, 6]],\n",
      "\n",
      "        [[1, 9, 7],\n",
      "         [3, 8, 1]],\n",
      "\n",
      "        [[8, 6, 1],\n",
      "         [4, 5, 2]]])\n"
     ]
    }
   ],
   "source": [
    "# permute(*dims): 重新排列所有维度\n",
    "# 参数: 新维度的排列顺序(必须包含所有维度)\n",
    "# 应用: 图像格式转换(CHW→HWC)、调整批次维度顺序\n",
    "# 示例: shape=[2,3,6], permute(2,0,1)→维度顺序变为[第2维,第0维,第1维]→shape=[6,2,3]\n",
    "tensor3 = tensor1.permute(2, 0, 1)  # 将维度重排为[6,2,3]\n",
    "print(\"原始形状:\", tensor1.shape)    # torch.Size([2, 3, 6])\n",
    "print(\"permute后形状:\", tensor3.shape) # torch.Size([6, 2, 3])\n",
    "print(\"permute后内容:\\n\", tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. 张量重塑操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reshape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.547828Z",
     "start_time": "2026-02-01T17:12:59.521282400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([2, 3, 6])\n",
      "reshape后形状: torch.Size([2, 18])\n",
      "reshape后内容:\n",
      " tensor([[3, 2, 9, 1, 1, 8, 3, 2, 9, 3, 9, 6, 7, 8, 2, 9, 7, 1],\n",
      "        [2, 5, 4, 5, 3, 4, 7, 7, 8, 3, 8, 5, 7, 7, 8, 6, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "# reshape(*shape): 改变张量形状(不改变元素顺序和总数)\n",
    "# 规则: 新形状的元素总数必须等于原形状元素总数\n",
    "# 应用: 全连接层输入展平、调整批次大小\n",
    "# 示例: [2,3,6]共36个元素→可reshape为[2,18]、[6,6]、[36]等\n",
    "# 技巧: 可用-1让PyTorch自动推断该维度大小\n",
    "tensor4 = tensor1.reshape(2, 18)  # 2x3x6=36元素→reshape为2x18\n",
    "print(\"原始形状:\", tensor1.shape)   # torch.Size([2, 3, 6])\n",
    "print(\"reshape后形状:\", tensor4.shape) # torch.Size([2, 18])\n",
    "print(\"reshape后内容:\\n\", tensor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reshape_auto",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.592948600Z",
     "start_time": "2026-02-01T17:12:59.548830900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape(3,-1)形状: torch.Size([3, 12])\n",
      "reshape(-1)形状: torch.Size([36])\n"
     ]
    }
   ],
   "source": [
    "# reshape使用-1自动推断维度\n",
    "# -1表示: 根据其他维度和总元素数自动计算该维度大小\n",
    "# 示例: [2,3,6]共36元素, reshape(3,-1)→PyTorch计算-1位置为36/3=12\n",
    "tensor5 = tensor1.reshape(3, -1)  # 自动计算为[3,12]\n",
    "print(\"reshape(3,-1)形状:\", tensor5.shape)  # torch.Size([3, 12])\n",
    "tensor6 = tensor1.reshape(-1)     # 展平为一维向量\n",
    "print(\"reshape(-1)形状:\", tensor6.shape)    # torch.Size([36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "view",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.618856Z",
     "start_time": "2026-02-01T17:12:59.593951800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor5是否内存连续: True\n",
      "view操作成功: torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "# view(*shape): 类似reshape,但要求张量内存连续\n",
    "# 区别: view要求内存连续(contiguous),reshape会在必要时复制数据\n",
    "# 性能: view更快(不复制数据),但有内存连续性要求\n",
    "# 检查: 用is_contiguous()检查内存是否连续\n",
    "tensor5 = tensor1  # 直接赋值,内存连续\n",
    "print(\"tensor5是否内存连续:\", tensor5.is_contiguous())  # True\n",
    "print(\"view操作成功:\", tensor5.view(2, 18).shape)      # torch.Size([2, 18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "view_contiguous",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.654147400Z",
     "start_time": "2026-02-01T17:12:59.621856900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor6是否内存连续: False\n",
      "contiguous后是否连续: True\n",
      "view操作成功: torch.Size([2, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_76828\\874988388.py:4: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4483.)\n",
      "  tensor6 = tensor1.T  # 转置后内存不连续\n"
     ]
    }
   ],
   "source": [
    "# 处理非连续内存的张量\n",
    "# transpose等操作会导致内存不连续,此时view会报错\n",
    "# 解决: 先调用contiguous()使内存连续,再使用view\n",
    "tensor6 = tensor1.T  # 转置后内存不连续\n",
    "print(\"tensor6是否内存连续:\", tensor6.is_contiguous())  # False\n",
    "# tensor6.view(6, 6)  # 这行会报错:RuntimeError\n",
    "\n",
    "# 使用contiguous()转为连续内存\n",
    "tensor6 = tensor6.contiguous()  # 复制数据使内存连续\n",
    "print(\"contiguous后是否连续:\", tensor6.is_contiguous())  # True\n",
    "print(\"view操作成功:\", tensor6.view(-1, 18).shape)      # 现在可以用view了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flatten",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.699913100Z",
     "start_time": "2026-02-01T17:12:59.655698100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten(1)后形状: torch.Size([2, 18])\n",
      "flatten()后形状: torch.Size([36])\n"
     ]
    }
   ],
   "source": [
    "# flatten(start_dim, end_dim): 展平指定维度范围\n",
    "# 参数: start_dim(起始维度), end_dim(结束维度,默认-1表示最后一维)\n",
    "# 应用: 卷积层输出展平后输入全连接层\n",
    "# 示例: [2,3,6]从dim=1开始展平→[2, 3*6]=[2,18]\n",
    "tensor7 = tensor1.flatten(start_dim=1)  # 保持dim0,展平后续维度\n",
    "print(\"flatten(1)后形状:\", tensor7.shape)  # torch.Size([2, 18])\n",
    "tensor8 = tensor1.flatten()  # 全部展平为一维\n",
    "print(\"flatten()后形状:\", tensor8.shape)   # torch.Size([36])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. 维度增减操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unsqueeze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.732871800Z",
     "start_time": "2026-02-01T17:12:59.706102200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([3])\n",
      "unsqueeze(0)形状: torch.Size([1, 3])\n",
      "unsqueeze(0)内容: tensor([[1, 2, 3]])\n",
      "unsqueeze(1)形状: torch.Size([3, 1])\n",
      "unsqueeze(1)内容:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze(dim): 在指定位置增加一个大小为1的维度\n",
    "# 参数: dim(插入位置,可为负数)\n",
    "# 应用: 增加batch维度、扩展维度以匹配广播规则\n",
    "# 示例: [3]→unsqueeze(0)→[1,3], unsqueeze(1)→[3,1]\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "print(\"原始形状:\", tensor1.shape)  # torch.Size([3])\n",
    "\n",
    "tensor2 = tensor1.unsqueeze(0)  # 在第0维插入\n",
    "print(\"unsqueeze(0)形状:\", tensor2.shape)  # torch.Size([1, 3]) - 行向量\n",
    "print(\"unsqueeze(0)内容:\", tensor2)\n",
    "\n",
    "tensor3 = tensor1.unsqueeze(1)  # 在第1维插入\n",
    "print(\"unsqueeze(1)形状:\", tensor3.shape)  # torch.Size([3, 1]) - 列向量\n",
    "print(\"unsqueeze(1)内容:\\n\", tensor3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unsqueeze_inplace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.757564200Z",
     "start_time": "2026-02-01T17:12:59.741538100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原地操作前: torch.Size([3])\n",
      "原地操作后: torch.Size([1, 3])\n",
      "tensor4现在是: tensor([[1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueeze_(): 原地操作版本(会修改原张量)\n",
    "# 区别: unsqueeze()返回新张量, unsqueeze_()直接修改原张量\n",
    "# 注意: 原地操作更节省内存,但会改变原张量\n",
    "tensor4 = torch.tensor([1, 2, 3])\n",
    "print(\"原地操作前:\", tensor4.shape)      # torch.Size([3])\n",
    "tensor4.unsqueeze_(dim=0)  # 直接修改tensor4\n",
    "print(\"原地操作后:\", tensor4.shape)      # torch.Size([1, 3])\n",
    "print(\"tensor4现在是:\", tensor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "squeeze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.780343100Z",
     "start_time": "2026-02-01T17:12:59.758308200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([1, 1, 3])\n",
      "squeeze()形状: torch.Size([3])\n",
      "squeeze(0)形状: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# squeeze(dim): 移除大小为1的维度\n",
    "# 参数: dim(可选,指定移除哪个维度;不指定则移除所有大小为1的维度)\n",
    "# 应用: 移除冗余维度、降维操作\n",
    "# 示例: [1,3,1]→squeeze()→[3], squeeze(0)→[3,1]\n",
    "tensor5 = torch.tensor([[[1, 2, 3]]])  # shape=[1,1,3]\n",
    "print(\"原始形状:\", tensor5.shape)  # torch.Size([1, 1, 3])\n",
    "\n",
    "tensor6 = tensor5.squeeze()  # 移除所有大小为1的维度\n",
    "print(\"squeeze()形状:\", tensor6.shape)  # torch.Size([3])\n",
    "\n",
    "tensor7 = tensor5.squeeze(0)  # 只移除第0维\n",
    "print(\"squeeze(0)形状:\", tensor7.shape)  # torch.Size([1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. 张量拼接操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.804958200Z",
     "start_time": "2026-02-01T17:12:59.782341600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沿dim=0拼接:\n",
      " tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "拼接后形状: torch.Size([4, 3])\n",
      "沿dim=1拼接:\n",
      " tensor([[ 1,  2,  3,  7,  8,  9],\n",
      "        [ 4,  5,  6, 10, 11, 12]])\n",
      "拼接后形状: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat(tensors, dim): 沿指定维度拼接张量\n",
    "# 参数: tensors(张量列表/元组), dim(拼接维度)\n",
    "# 要求: 除拼接维度外,其他维度大小必须相同\n",
    "# 应用: 合并批次数据、特征拼接\n",
    "# 示例: 两个[2,3]沿dim=0拼接→[4,3], 沿dim=1拼接→[2,6]\n",
    "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]])    # shape=[2,3]\n",
    "tensor2 = torch.tensor([[7, 8, 9], [10, 11, 12]]) # shape=[2,3]\n",
    "\n",
    "# 沿第0维(行)拼接 - 增加行数\n",
    "tensor3 = torch.cat((tensor1, tensor2), dim=0)\n",
    "print(\"沿dim=0拼接:\\n\", tensor3)\n",
    "print(\"拼接后形状:\", tensor3.shape)  # torch.Size([4, 3])\n",
    "\n",
    "# 沿第1维(列)拼接 - 增加列数\n",
    "tensor4 = torch.cat((tensor1, tensor2), dim=1)\n",
    "print(\"沿dim=1拼接:\\n\", tensor4)\n",
    "print(\"拼接后形状:\", tensor4.shape)  # torch.Size([2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stack",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.824028500Z",
     "start_time": "2026-02-01T17:12:59.806044200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沿dim=0堆叠形状: torch.Size([2, 4, 3, 5])\n",
      "沿dim=1堆叠形状: torch.Size([4, 2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack(tensors, dim): 沿新维度堆叠张量\n",
    "# 区别cat: cat在现有维度拼接, stack创建新维度\n",
    "# 要求: 所有张量形状必须完全相同\n",
    "# 应用: 批次数据组合、时序数据堆叠\n",
    "# 示例: 两个[4,3,5]沿dim=0堆叠→[2,4,3,5], 沿dim=1堆叠→[4,2,3,5]\n",
    "tensor1 = torch.randint(1, 10, [4, 3, 5])  # shape=[4,3,5]\n",
    "tensor2 = torch.randint(1, 10, [4, 3, 5])  # shape=[4,3,5]\n",
    "\n",
    "# 在第0维堆叠 - 创建新的批次维度\n",
    "tensor3 = torch.stack([tensor1, tensor2], dim=0)\n",
    "print(\"沿dim=0堆叠形状:\", tensor3.shape)  # torch.Size([2, 4, 3, 5])\n",
    "\n",
    "# 在第1维堆叠 - 在第1维位置插入新维度\n",
    "tensor4 = torch.stack([tensor1, tensor2], dim=1)\n",
    "print(\"沿dim=1堆叠形状:\", tensor4.shape)  # torch.Size([4, 2, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cat_vs_stack",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.850306600Z",
     "start_time": "2026-02-01T17:12:59.831042600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat结果: tensor([1, 2, 3, 4, 5, 6])\n",
      "cat结果形状: torch.Size([6])\n",
      "stack结果:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "stack结果形状: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# cat vs stack 对比示例\n",
    "# cat: 在现有维度上拼接,不增加维度数\n",
    "# stack: 创建新维度进行堆叠,维度数+1\n",
    "a = torch.tensor([1, 2, 3])  # shape=[3]\n",
    "b = torch.tensor([4, 5, 6])  # shape=[3]\n",
    "\n",
    "cat_result = torch.cat([a, b], dim=0)\n",
    "print(\"cat结果:\", cat_result)           # tensor([1,2,3,4,5,6])\n",
    "print(\"cat结果形状:\", cat_result.shape) # torch.Size([6])\n",
    "\n",
    "stack_result = torch.stack([a, b], dim=0)\n",
    "print(\"stack结果:\\n\", stack_result)     # tensor([[1,2,3],[4,5,6]])\n",
    "print(\"stack结果形状:\", stack_result.shape) # torch.Size([2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. 张量分割操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chunk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.878097400Z",
     "start_time": "2026-02-01T17:12:59.852754200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割成3块:\n",
      "  块0: tensor([0, 1, 2, 3])\n",
      "  块1: tensor([4, 5, 6, 7])\n",
      "  块2: tensor([8, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch.chunk(tensor, chunks, dim): 将张量分割为指定数量的块\n",
    "# 参数: tensor(待分割张量), chunks(分割块数), dim(分割维度)\n",
    "# 规则: 如果不能均分,最后一块会较小\n",
    "# 应用: 多GPU训练数据分配、大批次数据分块处理\n",
    "tensor1 = torch.arange(10)  # tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "chunks = torch.chunk(tensor1, 3, dim=0)  # 分成3块\n",
    "print(\"分割成3块:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"  块{i}: {chunk}\")  # [0,1,2,3], [4,5,6,7], [8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "split",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.907088900Z",
     "start_time": "2026-02-01T17:12:59.880636900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "固定大小分割:\n",
      "  块0: tensor([0, 1, 2])\n",
      "  块1: tensor([3, 4, 5])\n",
      "  块2: tensor([6, 7, 8])\n",
      "  块3: tensor([9])\n",
      "\n",
      "自定义大小分割:\n",
      "  块0: tensor([0, 1])\n",
      "  块1: tensor([2, 3, 4])\n",
      "  块2: tensor([5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch.split(tensor, split_size_or_sections, dim): 按指定大小分割\n",
    "# 参数: split_size_or_sections(整数或列表)\n",
    "#       - 整数: 每块的大小\n",
    "#       - 列表: 每块的具体大小\n",
    "# 应用: 按需分割数据、多任务学习特征分离\n",
    "tensor2 = torch.arange(10)\n",
    "\n",
    "# 按固定大小分割\n",
    "splits1 = torch.split(tensor2, 3, dim=0)  # 每块大小3\n",
    "print(\"固定大小分割:\")\n",
    "for i, s in enumerate(splits1):\n",
    "    print(f\"  块{i}: {s}\")  # [0,1,2], [3,4,5], [6,7,8], [9]\n",
    "\n",
    "# 按列表指定每块大小\n",
    "splits2 = torch.split(tensor2, [2, 3, 5], dim=0)  # 分别为2,3,5大小\n",
    "print(\"\\n自定义大小分割:\")\n",
    "for i, s in enumerate(splits2):\n",
    "    print(f\"  块{i}: {s}\")  # [0,1], [2,3,4], [5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. 张量重复操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "repeat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.922549Z",
     "start_time": "2026-02-01T17:12:59.908561800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "repeat(2,3)形状: torch.Size([4, 6])\n",
      "repeat后张量:\n",
      " tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4],\n",
      "        [1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# repeat(*sizes): 沿各维度重复张量\n",
    "# 参数: 各维度的重复次数(长度必须≥张量维度数)\n",
    "# 应用: 数据增强、广播操作的显式实现\n",
    "# 示例: [2,3]repeat(2,3)→沿dim0重复2次,dim1重复3次→[4,9]\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])  # shape=[2,2]\n",
    "tensor2 = tensor1.repeat(2, 3)  # 沿dim0重复2次,dim1重复3次\n",
    "print(\"原始张量:\\n\", tensor1)\n",
    "print(\"repeat(2,3)形状:\", tensor2.shape)  # torch.Size([4, 6])\n",
    "print(\"repeat后张量:\\n\", tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "repeat_interleave",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T17:12:59.943836200Z",
     "start_time": "2026-02-01T17:12:59.927057900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量: tensor([1, 2, 3])\n",
      "repeat_interleave(2): tensor([1, 1, 2, 2, 3, 3])\n",
      "\n",
      "原始矩阵:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "沿dim=0 repeat_interleave(2):\n",
      " tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# repeat_interleave(repeats, dim): 在指定维度上交错重复元素\n",
    "# 区别repeat: repeat整体重复, repeat_interleave逐元素重复\n",
    "# 应用: 上采样、标签扩展\n",
    "# 示例: [1,2,3]repeat_interleave(2)→[1,1,2,2,3,3]\n",
    "tensor3 = torch.tensor([1, 2, 3])\n",
    "tensor4 = tensor3.repeat_interleave(2)  # 每个元素重复2次\n",
    "print(\"原始张量:\", tensor3)  # tensor([1, 2, 3])\n",
    "print(\"repeat_interleave(2):\", tensor4)  # tensor([1, 1, 2, 2, 3, 3])\n",
    "\n",
    "# 二维张量的repeat_interleave\n",
    "tensor5 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor6 = tensor5.repeat_interleave(2, dim=0)  # 沿行重复\n",
    "print(\"\\n原始矩阵:\\n\", tensor5)\n",
    "print(\"沿dim=0 repeat_interleave(2):\\n\", tensor6)\n",
    "# 结果: [[1,2], [1,2], [3,4], [3,4]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
