# 第8章 循环神经网络

## 本章导读

循环神经网络（Recurrent Neural Network，RNN）是一类专门用于处理序列数据的神经网络。与前馈神经网络不同，RNN具有记忆能力，能够处理变长序列输入，在自然语言处理、语音识别、时间序列预测等领域有着广泛应用。

**学习目标**：
- 理解自然语言处理的基本概念和挑战
- 掌握词嵌入技术及其应用
- 理解RNN的工作原理和结构
- 熟悉LSTM和GRU等改进型RNN
- 能够使用PyTorch构建和训练RNN模型

**学习路线**：
```
NLP概述 → 词嵌入 → 循环网络 → 实战应用
(基本概念) (词向量) (RNN/LSTM) (古诗生成)
```

**核心概念**：
- 序列建模：处理时序和顺序数据
- 词嵌入：将词语映射到低维向量空间
- 循环连接：网络具有记忆能力
- 长短期记忆：解决长序列依赖问题

---

## 8.1 自然语言处理概述

自然语言处理（Natural Language Processing，NLP）是人工智能领域的重要分支，研究如何让计算机理解和处理人类语言。

### 8.1.1 基于同义词词典的方法

早期的NLP方法主要依赖人工构建的同义词词典和规则库。

**WordNet**：
- 普林斯顿大学开发的英语词汇数据库
- 包含词语的同义词集（synsets）
- 记录词语之间的语义关系（同义、反义、上下位等）

**优点**：
- 语义关系明确
- 可解释性强

**缺点**：
- 人工构建成本高
- 难以覆盖所有词汇
- 无法捕捉词语的细微差别
- 难以处理多义词

### 8.1.2 基于计数的方法

基于统计的方法通过分析语料库中词语的共现关系来表示词义。

**共现矩阵（Co-occurrence Matrix）**：

统计词语在固定窗口内共同出现的次数，构建词语-词语的共现矩阵。

**示例**：
```
语料库：
- 我 喜欢 深度 学习
- 深度 学习 很 有趣
- 我 喜欢 自然 语言 处理

窗口大小为2的共现矩阵：
        我    喜欢    深度    学习    自然    语言    处理
我      0      2       0       0       0       0       0
喜欢    2      0       1       0       1       0       0
深度    0      1       0       2       0       0       0
学习    0      0       2       0       0       0       0
...
```

**问题**：
- 矩阵维度高（词汇表大小×词汇表大小）
- 稀疏性问题
- 难以捕捉语义相似度

**改进方法**：
- **SVD（奇异值分解）**：对共现矩阵进行降维
- **PMI（点互信息）**：衡量词语关联强度

### 8.1.3 基于推理的方法

基于神经网络的方法通过预测任务学习词向量表示。

**Word2Vec**：

Google提出的词向量学习模型，包含两种架构：

1. **CBOW（Continuous Bag of Words）**：
   - 根据上下文预测中心词
   - 输入：上下文词语的词向量
   - 输出：中心词的概率分布

2. **Skip-gram**：
   - 根据中心词预测上下文
   - 输入：中心词的词向量
   - 输出：上下文词语的概率分布

**优点**：
- 自动学习词向量
- 捕捉语义和语法关系
- 支持词语类比（如：国王-男人+女人≈女王）

**GloVe（Global Vectors）**：

结合计数方法和推理方法的优点：
- 基于全局统计信息
- 使用神经网络训练
- 在词类比任务上表现优异

**FastText**：

Facebook提出的改进方法：
- 考虑子词信息（subword）
- 可以处理未登录词（OOV）
- 支持多种语言

---

## 8.2 词嵌入层

词嵌入（Word Embedding）是将离散的词语映射到连续的低维向量空间的技术。

### 8.2.1 什么是词嵌入

**核心思想**：
- 将每个词语表示为一个稠密的实数向量
- 语义相似的词语在向量空间中距离相近
- 向量维度通常在50-300之间

**词向量的特性**：

1. **语义相似性**：
   - 相似的词有相似的向量
   - 例如："国王"和"女王"的向量接近

2. **语义关系**：
   - 向量运算可以捕捉语义关系
   - 例如：vec("巴黎") - vec("法国") + vec("意大利") ≈ vec("罗马")

3. **降维**：
   - 将高维的one-hot表示压缩到低维空间
   - 减少参数量，提高计算效率

**One-hot vs Embedding**：

| 特性 | One-hot | Embedding |
|------|---------|-----------|
| 维度 | 词汇表大小（通常10k+） | 50-300 |
| 稀疏性 | 稀疏（只有一个1） | 稠密 |
| 语义信息 | 无 | 包含语义信息 |
| 计算效率 | 低 | 高 |

### 8.2.2 API使用

PyTorch提供了`nn.Embedding`层来实现词嵌入。

**主要参数**：

| 参数 | 说明 | 示例 |
|------|------|------|
| num_embeddings | 词汇表大小 | 10000 |
| embedding_dim | 嵌入维度 | 128 |
| padding_idx | 填充标记的索引 | 0 |
| max_norm | 嵌入向量的最大范数 | 1.0 |

**基础用法**：

```python
import torch
import torch.nn as nn

# 定义嵌入层
vocab_size = 10000      # 词汇表大小
embed_dim = 128         # 嵌入维度

embedding = nn.Embedding(vocab_size, embed_dim)

# 输入：词语索引（batch_size=4, seq_len=10）
input_indices = torch.randint(0, vocab_size, (4, 10))

# 前向传播
output = embedding(input_indices)

print(f"输入形状: {input_indices.shape}")   # torch.Size([4, 10])
print(f"输出形状: {output.shape}")          # torch.Size([4, 10, 128])
```

**预训练词向量**：

```python
# 加载预训练的词向量（如GloVe）
from torchtext.vocab import GloVe

# 使用GloVe预训练词向量
glove = GloVe(name='6B', dim=100)

# 获取词向量
vector = glove['word']  # torch.Size([100])
```

### 8.2.3 词嵌入实战示例

以下代码展示了完整的中文词嵌入流程，包括分词、构建词表、创建Embedding层和获取词向量。

**导入必要的库**：

```python
import torch
import torch.nn as nn
import jieba  # 结巴分词库，用于中文分词

# jieba分词原理：
# 1. 基于前缀词典实现高效的词图扫描
# 2. 生成句子中汉字所有可能成词情况所构成的有向无环图(DAG)
# 3. 采用动态规划查找最大概率路径，找出基于词频的最大切分组合
# 4. 对于未登录词，采用HMM模型进行识别
```

**中文分词与停用词过滤**：

```python
# 定义原始文本
text = "自然语言是由文字构成的，而语言的含义是由单词构成的。即单词是含义的最小单位。因此为了让计算机理解自然语言，首先要让它理解单词含义。"

# 使用jieba进行中文分词
# lcut = list cut，返回分词结果的列表
original_words = jieba.lcut(text)
print(original_words)
# 输出: ['自然语言', '是', '由', '文字', '构成', '的', '，', ...]

# 定义停用词表（高频但语义信息量少的词）
stopwords = {"的", "是", "而", "由", "，", "。"}

# 使用列表推导式过滤停用词
words = [word for word in original_words if word not in stopwords]
print(words)
# 输出: ['自然语言', '文字', '构成', '语言', '含义', '单词', ...]
```

**构建词表与Embedding层**：

```python
# 构建词表（id2word映射）
# 1. set(words): 将词列表转为集合，自动去除重复的词
# 2. list(...): 将集合转回列表，索引即字符ID
id2word = list(set(words))
print(id2word)
# 输出: ['自然语言', '最小', '计算机', '首先', '为了', '文字', ...]

# 构建反向映射（word2id字典）
# 使用字典推导式：{键表达式: 值表达式 for 变量 in 可迭代对象}
word2id = {word: i for i, word in enumerate(id2word)}
print(word2id)
# 输出: {'自然语言': 0, '最小': 1, '计算机': 2, ...}

# 创建词嵌入层
# num_embeddings: 词汇表大小（有多少个不同的词）
# embedding_dim: 词向量的维度（用多少个数字表示一个词）
embed = nn.Embedding(num_embeddings=len(id2word), embedding_dim=5)
```

**获取词向量**：

```python
# 遍历词表中的每个词，获取其对应的词向量
for id, word in enumerate(id2word):
    # torch.tensor(id): 将词的ID转换为PyTorch张量
    # embed(...): 调用Embedding层进行前向传播
    word_vec = embed(torch.tensor(id))
    
    # detach(): 从计算图中分离，不再追踪梯度
    # numpy(): 转换为NumPy数组，方便打印查看
    print(f"{id:>2}: {word:8}\t {word_vec.detach().numpy()}")

# 输出示例：
#  0: 自然语言    	 [ 0.53865093 -1.0054226  -0.0523538  -0.76718557  0.28203788]
#  1: 最小      	 [0.11028367 1.0005876  1.5475931  0.92961705 0.468658  ]
# ...
```

---

## 8.3 循环网络层

循环神经网络（RNN）是一类专门用于处理序列数据的神经网络，具有记忆能力，能够捕捉序列中的时序信息。

### 8.3.1 RNN层介绍

#### 基本RNN结构

**核心思想**：
- 网络在处理序列的每个时间步时，会接收当前输入和前一个时间步的隐藏状态
- 隐藏状态作为"记忆"，保存了之前的信息

**数学定义**：

$$
h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})
$$

其中：
- $x_t$：当前时间步的输入
- $h_{t-1}$：前一个时间步的隐藏状态
- $h_t$：当前时间步的隐藏状态
- $W_{ih}, W_{hh}$：权重矩阵
- $b_{ih}, b_{hh}$：偏置项

**RNN的展开结构**：

```
时间步1    时间步2    时间步3    ...    时间步T
   x1  →    x2  →    x3  →         →    xT
   ↓        ↓        ↓              ↓
  [RNN] →  [RNN] →  [RNN] →       →  [RNN]
   ↓        ↓        ↓              ↓
   h1       h2       h3             hT
   ↓        ↓        ↓              ↓
   y1       y2       y3             yT
```

#### RNN的变体

**1. 双向RNN（Bi-RNN）**：

同时考虑过去和未来的信息：
- 一个RNN从左到右处理序列
- 另一个RNN从右到左处理序列
- 将两个方向的隐藏状态拼接

**2. 深度RNN**：

堆叠多个RNN层：
- 增加网络的表达能力
- 能够学习更复杂的模式

#### RNN的问题

**梯度消失/爆炸**：

在长序列中，梯度在反向传播时会指数级减小或增大：
- 梯度消失：远距离的信息难以传递
- 梯度爆炸：参数更新不稳定

**长期依赖问题**：

难以捕捉长距离的依赖关系：
- 例如："我出生在法国，......，我会说流利的____"
- 需要记住"法国"才能预测"法语"

### 8.3.2 LSTM（长短期记忆网络）

LSTM通过引入门控机制解决RNN的长期依赖问题。

**核心组件**：

1. **遗忘门（Forget Gate）**：
   - 决定丢弃哪些信息
   - $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

2. **输入门（Input Gate）**：
   - 决定存储哪些新信息
   - $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
   - $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$

3. **细胞状态（Cell State）**：
   - 长期记忆的载体
   - $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$

4. **输出门（Output Gate）**：
   - 决定输出哪些信息
   - $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
   - $h_t = o_t \odot \tanh(C_t)$

**LSTM的优势**：
- 通过门控机制控制信息流
- 能够学习长期依赖关系
- 在各种序列任务中表现优异

### 8.3.3 GRU（门控循环单元）

GRU是LSTM的简化版本，合并了细胞状态和隐藏状态。

**核心组件**：

1. **更新门（Update Gate）**：
   - 决定保留多少旧信息，接受多少新信息
   - $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$

2. **重置门（Reset Gate）**：
   - 决定丢弃多少旧信息
   - $r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$

3. **候选隐藏状态**：
   - $\tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t])$

4. **隐藏状态更新**：
   - $h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$

**GRU vs LSTM**：

| 特性 | GRU | LSTM |
|------|-----|------|
| 门数量 | 2个（更新门、重置门） | 3个（遗忘门、输入门、输出门） |
| 状态数量 | 1个（隐藏状态） | 2个（隐藏状态、细胞状态） |
| 参数量 | 较少 | 较多 |
| 性能 | 相当 | 相当 |
| 训练速度 | 较快 | 较慢 |

### 8.3.4 API使用

PyTorch提供了`nn.RNN`、`nn.LSTM`、`nn.GRU`来实现循环网络。

**主要参数**：

| 参数 | 说明 | 默认值 |
|------|------|--------|
| input_size | 输入特征维度 | 必需 |
| hidden_size | 隐藏状态维度 | 必需 |
| num_layers | RNN层数 | 1 |
| batch_first | 是否batch维度在前 | False |
| dropout | dropout概率 | 0 |
| bidirectional | 是否双向 | False |

**RNN基础用法**：

```python
import torch
import torch.nn as nn

# 定义RNN
rnn = nn.RNN(
    input_size=100,      # 输入维度（词嵌入维度）
    hidden_size=128,     # 隐藏状态维度
    num_layers=2,        # 2层RNN
    batch_first=True,    # batch维度在前
    dropout=0.5          # dropout概率
)

# 输入：(batch_size, seq_len, input_size)
input_seq = torch.randn(32, 50, 100)  # batch=32, seq_len=50, input_dim=100

# 前向传播
output, hidden = rnn(input_seq)

print(f"输出形状: {output.shape}")    # torch.Size([32, 50, 128])
print(f"隐藏状态形状: {hidden.shape}") # torch.Size([2, 32, 128])
```

**LSTM用法**：

```python
# 定义LSTM
lstm = nn.LSTM(
    input_size=100,
    hidden_size=128,
    num_layers=2,
    batch_first=True,
    dropout=0.5,
    bidirectional=True  # 双向LSTM
)

# 输入
input_seq = torch.randn(32, 50, 100)

# 前向传播（LSTM返回输出、隐藏状态和细胞状态）
output, (hidden, cell) = lstm(input_seq)

print(f"输出形状: {output.shape}")    # torch.Size([32, 50, 256])（双向，维度翻倍）
print(f"隐藏状态形状: {hidden.shape}") # torch.Size([4, 32, 128])（2层×2方向）
```

**GRU用法**：

```python
# 定义GRU
gru = nn.GRU(
    input_size=100,
    hidden_size=128,
    num_layers=1,
    batch_first=True
)

# 输入
input_seq = torch.randn(32, 50, 100)

# 前向传播
output, hidden = gru(input_seq)

print(f"输出形状: {output.shape}")    # torch.Size([32, 50, 128])
print(f"隐藏状态形状: {hidden.shape}") # torch.Size([1, 32, 128])
```

### 8.3.5 RNN实战示例

以下代码详细展示了PyTorch中RNN的使用方法，包括创建RNN层、准备输入数据、执行前向传播和理解输出。

**创建RNN层**：

```python
import torch
import torch.nn as nn

# 创建一个两层的RNN网络
# input_size=8:  每个时间步的输入特征维度为8
# hidden_size=16: 隐状态的维度为16(也是输出维度)
# num_layers=2:   堆叠2层RNN,增强模型表达能力

rnn = nn.RNN(input_size=8, hidden_size=16, num_layers=2)

# RNN的结构:
# 输入(8维) -> RNN第1层(16维隐状态) -> RNN第2层(16维隐状态) -> 输出(16维)
# 每一层都维护自己的隐状态,层与层之间隐状态独立
```

**准备输入数据**：

```python
# 1. 创建输入序列
# 形状:(seq_len=3, batch=3, input_size=8)
# - 序列长度为3:表示有3个时间步(例如句子有3个词)
# - 批次大小为3:同时处理3个样本(例如3个句子)
# - 特征维度为8:每个时间步的输入是8维向量
input_seq = torch.randn(3, 3, 8)

# 2. 创建初始隐状态
# 形状:(num_layers=2, batch=3, hidden_size=16)
# - 层数为2:因为RNN有2层,每层都需要初始隐状态
# - 批次大小为3:必须与input的batch一致
# - 隐状态维度为16:必须与RNN定义的hidden_size一致
hx = torch.randn(2, 3, 16)

print(input_seq.shape)   # 输出: torch.Size([3, 3, 8])
print(hx.shape)          # 输出: torch.Size([2, 3, 16])
```

**执行前向传播**：

```python
# 调用RNN进行前向传播
# 输入: input(3,3,8) - 输入序列
#      hx(2,3,16) - 初始隐状态
# 输出: output - 每个时间步的输出
#      hn - 最后一个时间步的隐状态
output, hn = rnn(input_seq, hx)

print(output.shape)  # 输出: torch.Size([3, 3, 16])
print(hn.shape)      # 输出: torch.Size([2, 3, 16])
```

**输出解释**：

- **output形状**: `(seq_len=3, batch=3, hidden_size=16)`
  - 包含所有3个时间步的输出
  - `output[0]`: 第1个时间步，所有样本的输出
  - `output`实际上是最后一层RNN在每个时间步的隐状态

- **hn形状**: `(num_layers=2, batch=3, hidden_size=16)`
  - 包含最后一个时间步每一层的隐状态
  - `hn[0]`: 第1层RNN在最后时间步的隐状态
  - `hn[1]`: 第2层RNN在最后时间步的隐状态
  - **注意**: `hn[-1]`(最后一层的最后隐状态)等于`output[-1]`(最后时间步的输出)

**内部计算流程**（简化理解）：

```
时间步0: input[0] + hx[0] -> h1_0 (第1层)
                 h1_0 + hx[1] -> h2_0 (第2层) -> output[0]

时间步1: input[1] + h1_0 -> h1_1 (第1层)
                 h1_1 + h2_0 -> h2_1 (第2层) -> output[1]

时间步2: input[2] + h1_1 -> h1_2 (第1层) -> hn[0]
                 h1_2 + h2_1 -> h2_2 (第2层) -> output[2] = hn[1]
```

---

## 8.4 案例：古诗生成

本节将使用RNN实现一个古诗生成模型，学习如何使用循环神经网络进行文本生成任务。

### 8.4.1 数据预处理

古诗生成的数据预处理步骤包括读取文本、清洗数据、构建词表和序列化转换。

```python
import re

def preprocess_poems(file_path):
    """
    古诗数据预处理函数
    读取原始古诗文本，进行数据清洗、字符分割、词表构建和序列化转换
    """
    char_set = set()
    poems = []
    
    # 1. 读取数据
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # 数据清洗：使用正则表达式去除中文标点
            line = re.sub(r"[，。？！、：]", "", line).strip()
            # 按字分割，去重保存到set
            char_set.update(list(line))
            poems.append(list(line))
    
    # 2. 构建词表
    # id2word: 索引到字符的映射列表
    # 添加'<UNK>'标记用于处理未登录词
    id2word = list(char_set) + ['<UNK>']
    # word2id: 字符到索引的映射字典
    word2id = {word: id for id, word in enumerate(id2word)}
    
    # 3. 将诗句id化
    id_seqs = []
    for poem in poems:
        id_seq = [word2id.get(word) for word in poem]
        id_seqs.append(id_seq)
    
    return id_seqs, id2word, word2id

# 执行数据预处理
id_seqs, id2word, word2id = preprocess_poems('../data/poems.txt')
```

### 8.4.2 自定义Dataset

使用PyTorch的Dataset和DataLoader加载古诗数据，采用滑动窗口机制生成训练样本。

```python
from torch.utils.data import Dataset
import torch

class PoetryData(Dataset):
    """
    古诗数据集类
    采用滑动窗口机制将长序列切分为固定长度的训练样本对(input, target)
    """
    
    def __init__(self, id_seqs, seq_len):
        """
        参数:
            id_seqs: 所有诗的ID序列列表
            seq_len: 序列长度L，即输入序列的固定长度
        """
        self.id_seqs = id_seqs
        self.seq_len = seq_len
        self.data = []  # 保存元组（X,y）的列表
        
        # 遍历所有诗，使用滑动窗口生成训练样本
        # 对于序列 [c1, c2, c3, c4, c5, c6] 和 seq_len=3：
        # - 样本1: X=[c1,c2,c3], y=[c2,c3,c4] (预测下一个字符)
        # - 样本2: X=[c2,c3,c4], y=[c3,c4,c5]
        for id_seq in id_seqs:
            for i in range(0, len(id_seq) - self.seq_len):
                self.data.append((
                    id_seq[i:i + self.seq_len],           # 输入序列
                    id_seq[i + 1:i + 1 + self.seq_len]    # 目标序列(右移一位)
                ))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = torch.LongTensor(self.data[idx][0])
        y = torch.LongTensor(self.data[idx][1])
        return x, y

# 创建数据集实例，序列长度设置为24
dataset = PoetryData(id_seqs, seq_len=24)
```

### 8.4.3 搭建模型

构建基于RNN的古诗生成模型，模型由Embedding层、RNN层和全连接层组成。

```python
import torch.nn as nn

class PoetryRNN(nn.Module):
    """
    古诗生成RNN模型
    架构: 输入(字符ID) -> Embedding -> RNN -> Linear -> 输出(词表概率分布)
    """
    
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):
        """
        参数:
            vocab_size: 词汇表大小
            embedding_dim: 嵌入维度，每个字符用多少维向量表示
            hidden_size: RNN隐藏状态维度
            num_layers: RNN层数
        """
        super(PoetryRNN, self).__init__()
        # 字符嵌入层：将字符ID映射为稠密向量
        self.embed = nn.Embedding(vocab_size, embedding_dim)
        # RNN层：学习序列的时序依赖关系
        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)
        # 全连接层：将RNN输出映射为词表大小的概率分布
        self.linear = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, input, hx=None):
        """
        前向传播
        参数:
            input: 输入字符ID序列，形状 (batch_size, seq_len)
            hx: 初始隐藏状态，形状 (num_layers, batch_size, hidden_size)
        返回:
            output: 每个位置对每个字符的预测分数，形状 (batch, seq_len, vocab_size)
            hn: 最后一个时间步的隐藏状态
        """
        embed = self.embed(input)
        output, hn = self.rnn(embed, hx)
        output = self.linear(output)
        return output, hn

# 创建模型实例
model = PoetryRNN(
    vocab_size=len(id2word),
    embedding_dim=256,
    hidden_size=512,
    num_layers=2
)
```

### 8.4.4 模型训练

实现古诗生成模型的训练流程，包括定义损失函数、优化器和训练循环。

```python
from torch.utils.data import DataLoader
import torch.optim as optim

def train(model, dataset, lr, epoch_num, batch_size, device):
    """
    模型训练函数
    参数:
        model: 待训练的模型
        dataset: 训练数据集
        lr: 学习率
        epoch_num: 训练轮数
        batch_size: 批次大小
        device: 计算设备(CPU或CUDA)
    """
    # 初始化
    model.to(device)
    model.train()
    
    # 定义损失函数和优化器
    loss_fn = nn.CrossEntropyLoss()  # 交叉熵损失
    optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam优化器
    
    # 迭代训练
    for epoch in range(epoch_num):
        train_loss = 0
        # 定义数据加载器
        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        for batch_idx, (x, y) in enumerate(data_loader):
            x, y = x.to(device), y.to(device)
            
            # 前向传播
            output, hn = model(x)
            
            # 计算损失
            # output.transpose(1, 2): 调整维度以匹配CrossEntropyLoss要求
            # 原始: (batch, seq_len, vocab_size) -> 目标: (batch, vocab_size, seq_len)
            loss_value = loss_fn(output.transpose(1, 2), y)
            
            # 反向传播
            loss_value.backward()
            
            # 更新参数
            optimizer.step()
            
            # 梯度清零
            optimizer.zero_grad()
            
            train_loss += loss_value.item() * x.shape[0]
            
            # 打印训练进度条
            progress = int((batch_idx + 1) / len(data_loader) * 50)
            print(f"\rEpoch:{epoch + 1:0>2}[{'=' * progress}]{' ' * (50 - progress)}]", end="")
        
        # 本轮训练结束，计算并打印平均损失
        this_loss = train_loss / len(dataset)
        print(f" train_loss:{this_loss:.4f}")

# 设备配置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 超参数
lr = 1e-3           # 学习率
epoch_num = 20      # 训练轮数
batch_size = 32     # 批次大小

# 执行训练
train(model, dataset, lr, epoch_num, batch_size, device)
```

### 8.4.5 古诗生成

使用训练好的模型生成古诗，采用自回归生成策略和随机采样。

```python
def generate_poem(model, id2word, word2id, start_token, line_num=4, line_length=7):
    """
    古诗生成函数
    使用自回归生成策略，从给定的起始字符开始逐字生成诗句
    
    参数:
        model: 训练好的模型
        id2word: 索引到字符的映射列表
        word2id: 字符到索引的映射字典
        start_token: 起始字符
        line_num: 生成诗的行数，默认4行（绝句）
        line_length: 每行的字数，默认7字（七言）
    返回:
        生成的完整古诗字符串
    """
    model.eval()  # 设置评估模式
    poem = []
    current_rest_line = line_length
    
    # 起始字符ID化
    start_id = word2id.get(start_token, word2id['<UNK>'])
    if start_id != word2id['<UNK>']:
        poem.append(start_token)
        current_rest_line -= 1
    
    # 定义输入数据，形状 (1, 1)
    input_tensor = torch.LongTensor([[start_id]]).to(device)
    
    # 迭代生成诗句
    with torch.no_grad():  # 禁用梯度计算
        for i in range(line_num):
            # 生成两句诗（每行由两个半句组成）
            for interpunction in ["，", "。\n"]:
                while current_rest_line > 0:
                    # 前向传播
                    output, hn = model(input_tensor)
                    
                    # 得到概率分布
                    prob = torch.softmax(output[0, 0], dim=-1)
                    
                    # 基于概率分布随机采样下一个字符
                    next_id = torch.multinomial(prob, num_samples=1)
                    
                    # 添加到生成结果
                    poem.append(id2word[next_id.item()])
                    current_rest_line -= 1
                    
                    # 更新输入为新生成的字符
                    input_tensor = next_id.unsqueeze(0)
                
                # 添加标点
                poem.append(interpunction)
                current_rest_line = line_length
    
    return "".join(poem)

# 生成10首古诗
for i in range(10):
    print(generate_poem(
        model, id2word, word2id,
        start_token="一",
        line_length=7,
        line_num=4
    ))
```

**生成策略说明**：

1. **自回归生成**：每个新生成的字符作为下一个预测的输入，保证序列连贯性
2. **随机采样**：使用`torch.multinomial`按概率分布采样，而非贪婪选择，增加多样性
3. **格式控制**：支持指定行数和每行字数，自动添加中文标点

---

## 本章小结

### 核心概念回顾

1. **自然语言处理**：
   - 从基于规则到基于统计再到基于神经网络的发展
   - 词向量表示是NLP的基础

2. **词嵌入技术**：
   - One-hot表示：高维稀疏，无语义信息
   - 分布式表示：低维稠密，包含语义信息
   - Word2Vec、GloVe、FastText等经典方法

3. **循环神经网络**：
   - RNN：基础循环结构，存在梯度消失问题
   - LSTM：引入门控机制，解决长期依赖
   - GRU：简化版LSTM，参数更少，效果相当

4. **序列建模应用**：
   - 文本分类
   - 机器翻译
   - 文本生成
   - 语音识别

### RNN家族对比

| 模型 | 特点 | 适用场景 |
|------|------|---------|
| RNN | 结构简单，参数少 | 短序列任务 |
| LSTM | 门控机制，解决长期依赖 | 长序列任务，默认首选 |
| GRU | 简化LSTM，训练快 | 资源受限，效果相当 |
| Bi-RNN | 双向信息 | 需要上下文信息的任务 |

### 最佳实践

1. **词嵌入选择**：
   - 使用预训练词向量（GloVe、Word2Vec）
   - 在特定领域数据上微调
   - 维度通常选择100-300

2. **RNN架构选择**：
   - 默认使用LSTM或GRU
   - 长序列优先使用LSTM
   - 需要速度优先选择GRU

3. **训练技巧**：
   - 使用梯度裁剪防止梯度爆炸
   - 使用dropout防止过拟合
   - 使用学习率衰减

4. **序列处理**：
   - 使用pack_padded_sequence处理变长序列
   - 注意batch_first参数的设置
   - 合理设置序列长度

### 下一步学习

掌握了本章内容后，建议继续学习：
- 注意力机制（Attention）
- Transformer架构
- BERT、GPT等预训练语言模型
- 序列到序列模型（Seq2Seq）
- 机器翻译和文本摘要

通过系统学习这些内容，你将能够使用深度学习解决各种复杂的自然语言处理问题。
