# 第3章 反向传播算法

## 本章导读

第2章中，我们使用数值微分法计算梯度，但这种方法计算量巨大、耗时长。反向传播算法是一种高效计算梯度的方法，是训练深度神经网络的核心技术。

**学习目标**：
- 理解计算图和链式法则
- 掌握各种层的反向传播方法
- 能够实现完整的反向传播网络
- 理解反向传播相比数值微分的优势

**效率对比**：
```
数值微分：
- 每个参数需要计算 2 次前向传播
- 1000 个参数 = 2000 次计算
- 时间复杂度：O(n)

反向传播：
- 1 次前向传播 + 1 次反向传播
- 1000 个参数 = 2 次计算
- 时间复杂度：O(1)

速度提升：500倍！
```

**学习路线**：
```
计算图 → 链式法则 → 基本节点 → 激活层 → Affine层 → 输出层 → 完整网络
(可视化)  (数学基础)  (加乘法)  (ReLU/Sigmoid) (全连接)  (Softmax)  (综合实现)
```

---

## 3.1 计算图

### 3.1.1 什么是计算图

**定义**：计算图是用图形表示计算过程的一种方法，它将计算过程用节点和边组成的图结构来表示。

**计算图的组成**：
- **节点（Node）**：表示运算操作（如加法、乘法、激活函数等）
- **边（Edge）**：表示数据流动，连接各个运算节点

### 3.1.2 计算图的优势

**1. 局部计算**：
- 无论全局计算多么复杂，都可以将其分解为简单的局部计算
- 每个节点只需关注与自己相关的计算
- 最终结果由各个节点的局部计算结果传递得到

**示例**：计算 `z = (x + y) * 2`
```
x → [+] → a → [×2] → z
y ↗         
```
- 第一个节点：计算加法 `a = x + y`
- 第二个节点：计算乘法 `z = a * 2`

**2. 高效计算梯度**：
- 利用链式法则，计算图可以高效地计算导数
- 通过反向传播，可以一次性计算出所有参数的梯度
- 避免重复计算，提高效率

**3. 可视化计算过程**：
- 直观展示数据流动和计算依赖关系
- 便于理解复杂的神经网络结构
- 帮助调试和优化网络

### 3.1.3 计算图的正向传播和反向传播

**正向传播（Forward Propagation）**：
- 从输入开始，按照计算图的方向依次计算每个节点
- 最终得到输出结果
- 用于推理和预测

**反向传播（Backward Propagation）**：
- 从输出开始，反向计算梯度
- 利用链式法则传递导数
- 用于训练和参数更新

**示例**：`z = x * y` 的正向传播和反向传播

正向传播：
```
x = 2, y = 3
z = x * y = 6
```

反向传播（假设 ∂L/∂z = 1）：
```
∂L/∂x = ∂L/∂z × ∂z/∂x = 1 × y = 3
∂L/∂y = ∂L/∂z × ∂z/∂y = 1 × x = 2
```

---

## 3.2 链式法则

### 3.2.1 链式法则的定义

**数学定义**：如果函数 y 是 u 的函数，u 是 x 的函数，则 y 关于 x 的导数为：

$$
\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx}
$$

**扩展形式**（多层复合）：

$$
\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dv} \times \frac{dv}{dx}
$$

### 3.2.2 链式法则在神经网络中的应用

在神经网络中，链式法则用于计算损失函数关于各层参数的梯度。

**示例**：两层神经网络

```
输入 x → 线性变换 z₁ = W₁x + b₁ → 激活 a₁ = σ(z₁) → 线性变换 z₂ = W₂a₁ + b₂ → 输出 y
```

计算 ∂L/∂W₁（损失函数对第一层权重的梯度）：

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial z_2} \times \frac{\partial z_2}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial W_1}
$$

**重要说明**：
- 链式法则是反向传播算法的数学基础
- 每一层的梯度都依赖于后一层传回的梯度
- 梯度从输出层逐层反向传播到输入层

### 3.2.3 链式法则的计算示例

**示例 1**：简单复合函数

函数：`y = (x + 1)²`

拆解：`u = x + 1`, `y = u²`

计算导数：
$$
\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx} = 2u \times 1 = 2(x + 1)
$$

**示例 2**：多变量复合函数

函数：`L = (x + y)²`

设 `z = x + y`, 则 `L = z²`

计算偏导数：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial x} = 2z \times 1 = 2(x + y)
$$

$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial y} = 2z \times 1 = 2(x + y)
$$

**关键要点**：
- 链式法则将复杂的导数计算分解为简单的局部导数乘积
- 每个中间变量只需计算自己的局部导数
- 最终梯度通过层层相乘得到
- 这种“分而治之”的思想是反向传播的精髓

---

## 3.3 反向传播

### 3.3.1 加法节点的反向传播

**正向传播**：
$$
z = x + y
$$

**反向传播**：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial x} = \frac{\partial L}{\partial z} \times 1 = \frac{\partial L}{\partial z}
$$

$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial y} = \frac{\partial L}{\partial z} \times 1 = \frac{\partial L}{\partial z}
$$

**重要特性**：
- 加法节点的反向传播直接将上游梯度传递给下游
- 不改变梯度的值
- 相当于梯度分发器，将同一个梯度分发给所有输入

**直观理解**：
- 如果 z 增加 1，x 和 y 各增加 1 也能使 z 增加 1
- 因此，上游传来的梯度会原封不动地传递给 x 和 y

### 3.3.2 乘法节点的反向传播

**正向传播**：
$$
z = x \times y
$$

**反向传播**：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial x} = \frac{\partial L}{\partial z} \times y
$$

$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \times \frac{\partial z}{\partial y} = \frac{\partial L}{\partial z} \times x
$$

**重要特性**：
- 乘法节点反向传播时，需要交换输入值
- 传给 x 的梯度是上游梯度乘以 y 的值
- 传给 y 的梯度是上游梯度乘以 x 的值

**直观理解**：
- x 对 z 的影响取决于 y 的大小
- 如果 y 很大，x 的微小变化会引起 z 的巨大变化
- 反之亦然

**计算示例**：

已知：x = 2, y = 3, z = 6, ∂L/∂z = 1.5

反向传播：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \times y = 1.5 \times 3 = 4.5
$$

$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \times x = 1.5 \times 2 = 3.0
$$

### 3.3.3 梯度传播

**梯度传播的本质**：
- 梯度表示"变化率"或"敏感度"
- 反向传播就是将这种敏感度从输出逐层传回输入
- 每个节点根据局部导数调整传递的梯度

**多层传播示例**：

计算图：`x → [×2] → a → [+3] → b → [×5] → y`

正向传播（x = 1）：
$$
a = x \times 2 = 2
$$

$$
b = a + 3 = 5
$$

$$
y = b \times 5 = 25
$$

反向传播（∂L/∂y = 1）：
$$
\frac{\partial L}{\partial b} = \frac{\partial L}{\partial y} \times \frac{\partial y}{\partial b} = 1 \times 5 = 5
$$

$$
\frac{\partial L}{\partial a} = \frac{\partial L}{\partial b} \times \frac{\partial b}{\partial a} = 5 \times 1 = 5
$$

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial a} \times \frac{\partial a}{\partial x} = 5 \times 2 = 10
$$

**重要说明**：
- 反向传播的顺序与正向传播相反
- 每个节点只需知道：上游传来的梯度 + 本节点的局部导数
- 不需要知道整个网络的结构

**实用技巧**：
在实现反向传播时，记住三个核心原则：
1. **局部计算**：每个节点只关注自己的输入和输出
2. **信息保存**：正向传播时保存必要的中间值
3. **梯度传递**：上游梯度 × 局部导数 = 下游梯度

---

## 3.4 激活层的反向传播和实现

### 3.4.1 ReLU的反向传播

**ReLU 函数定义**：
$$
ReLU(x) = \max(0, x) = \begin{cases}
x, & \text{if } x > 0 \\
0, & \text{if } x \leq 0
\end{cases}
$$

**ReLU 导数**：
$$
\frac{\partial ReLU}{\partial x} = \begin{cases}
1, & \text{if } x > 0 \\
0, & \text{if } x \leq 0
\end{cases}
$$

**反向传播规则**：
$$
\frac{\partial L}{\partial x} = \begin{cases}
\frac{\partial L}{\partial y}, & \text{if } x > 0 \text{ (正向传播时 } x > 0\text{)} \\
0, & \text{if } x \leq 0 \text{ (正向传播时 } x \leq 0\text{)}
\end{cases}
$$

**直观理解**：
- 当正向传播时 x > 0，梯度直接传递（乘以1）
- 当正向传播时 x ≤ 0，梯度被阻断（乘以0）
- ReLU 像一个开关，只让正值的梯度通过

**实现要点**：
- 正向传播时需要记录输入 x 的符号（mask）
- 反向传播时根据 mask 决定是否传递梯度

**代码实现**（来自 `common/layers.py`）：
```python
# ReLu
class Relu:
    # 初始化
    def __init__(self):
        # mask: 用于保存输入数据中小于等于0的元素的索引
        self.mask = None

    # 前向传播
    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        # 将输入数据中小于等于0的元素设为0
        out[self.mask] = 0
        return out

    # 反向传播
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx
```

**代码说明**：
- **mask 机制**：使用布尔数组记录哪些位置的输入 ≤ 0
- **前向传播**：复制输入并将负值和零值位置设为 0
- **反向传播**：根据 mask 将对应位置的梯度设为 0，其他位置梯度直接传递
- **向量化运算**：使用 NumPy 的布尔索引，高效处理整个批量

### 3.4.2 Sigmoid的反向传播

**Sigmoid 函数定义**：
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**Sigmoid 导数**：
$$
\frac{\partial \sigma}{\partial x} = \sigma(x) \times (1 - \sigma(x))
$$

**重要性质**：
- Sigmoid 的导数可以用其输出值表示
- 无需重新计算指数，效率更高

**反向传播规则**：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \times \frac{\partial y}{\partial x} = \frac{\partial L}{\partial y} \times y \times (1 - y)
$$

其中 y = σ(x) 是正向传播的输出。

**直观理解**：
- Sigmoid 导数在 x=0 附近最大（约0.25）
- 在 x 很大或很小时，导数接近0（梯度消失）
- 这就是为什么深层网络常用 ReLU 而非 Sigmoid

**计算示例**：

正向传播：x = 0, y = σ(0) = 0.5

反向传播（∂L/∂y = 1）：
$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \times y \times (1 - y) = 1 \times 0.5 \times 0.5 = 0.25
$$

**实现要点**：
- 正向传播时保存输出 y
- 反向传播时用 y × (1-y) 计算局部梯度

**代码实现**（来自 `common/layers.py`）：
```python
# Sigmoid
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = sigmoid(x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

**代码说明**：
- **保存输出**：`self.out` 存储 Sigmoid 的输出值 y = σ(x)
- **前向传播**：调用 sigmoid 函数并保存结果
- **反向传播**：直接使用公式 dx = dout × y × (1-y)
- **效率优化**：避免重复计算指数，直接使用保存的输出值

---

## 3.5 Affine的反向传播和实现

### 3.5.1 什么是Affine层

**定义**：Affine层（仿射层）是神经网络中的全连接层，执行线性变换。

**数学表达式**：
$$
Y = X \cdot W + b
$$

其中：
- X: 输入矩阵，形状 (batch_size, input_size)
- W: 权重矩阵，形状 (input_size, output_size)
- b: 偏置向量，形状 (output_size,) 或 (1, output_size)
- Y: 输出矩阵，形状 (batch_size, output_size)
- @: 矩阵乘法运算符

### 3.5.2 Affine层的正向传播

**单样本情况**：
$$
输入: x \in \mathbb{R}^n
$$

$$
权重: W \in \mathbb{R}^{n \times m}
$$

$$
偏置: b \in \mathbb{R}^m
$$

$$
输出: y = x \cdot W + b \in \mathbb{R}^m
$$

**批量处理情况**：
$$
输入: X \in \mathbb{R}^{B \times n} \text{ (B个样本)}
$$

$$
权重: W \in \mathbb{R}^{n \times m}
$$

$$
偏置: b \in \mathbb{R}^m
$$

$$
输出: Y = X \cdot W + b \in \mathbb{R}^{B \times m}
$$

**广播机制**：
- 偏置 b 会自动广播到每个样本
- 相当于每一行都加上 b

### 3.5.3 Affine层的反向传播

**梯度推导**：

给定上游梯度 ∂L/∂Y，需要计算：
1. ∂L/∂X（传给前一层）
2. ∂L/∂W（更新权重）
3. ∂L/∂b（更新偏置）

**矩阵形式的梯度**：

$$
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T
$$

$$
\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}
$$

$$
\frac{\partial L}{\partial b} = \sum\left(\frac{\partial L}{\partial Y}, \text{axis}=0\right)
$$

**形状分析**：

假设：
- X: (B, n)
- W: (n, m)
- Y: (B, m)
- ∂L/∂Y: (B, m)

则：
$$
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T = (B, m) \cdot (m, n) = (B, n) \checkmark
$$

$$
\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y} = (n, B) \cdot (B, m) = (n, m) \checkmark
$$

$$
\frac{\partial L}{\partial b} = \sum\left(\frac{\partial L}{\partial Y}, \text{axis}=0\right) = (m,) \checkmark
$$

**重要说明**：
- ∂L/∂b 需要对 batch 维度求和，因为每个样本都对 b 有贡献
- 矩阵转置的使用是为了保证维度匹配
- 这些公式可以通过链式法则和矩阵求导推导得出

**直观理解**：

1. **∂L/∂X = ∂L/∂Y @ Wᵀ**
   - 输出的梯度通过权重矩阵的转置传回输入
   - W 的每一列影响输出的一个维度，因此需要转置

2. **∂L/∂W = Xᵀ @ ∂L/∂Y**
   - 权重的梯度是输入和输出梯度的外积
   - 每个权重的梯度取决于对应的输入和输出梯度

3. **∂L/∂b = sum(∂L/∂Y, axis=0)**
   - 偏置对所有样本共享，因此梯度是所有样本的总和

### 3.5.4 计算示例

**正向传播示例**：

$$
输入: X = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \quad (2 \times 2)
$$

$$
权重: W = \begin{bmatrix} 0.5 & 0.8 \\ 0.2 & 0.3 \end{bmatrix} \quad (2 \times 2)
$$

$$
偏置: b = [0.1, 0.1] \quad (2,)
$$

$$
计算:
$$

$$
Y = X \cdot W + b
$$

$$
= \begin{bmatrix} 1 \times 0.5 + 2 \times 0.2 & 1 \times 0.8 + 2 \times 0.3 \\ 3 \times 0.5 + 4 \times 0.2 & 3 \times 0.8 + 4 \times 0.3 \end{bmatrix} + \begin{bmatrix} 0.1 & 0.1 \\ 0.1 & 0.1 \end{bmatrix}
$$

$$
= \begin{bmatrix} 0.9 & 1.5 \\ 2.3 & 3.5 \end{bmatrix}
$$

**反向传播示例**：

$$
上游梯度: \frac{\partial L}{\partial Y} = \begin{bmatrix} 1.0 & 1.5 \\ 2.0 & 2.5 \end{bmatrix} \quad (2 \times 2)
$$

$$
计算:
$$

$$
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T
$$

$$
= \begin{bmatrix} 1.0 & 1.5 \\ 2.0 & 2.5 \end{bmatrix} \cdot \begin{bmatrix} 0.5 & 0.2 \\ 0.8 & 0.3 \end{bmatrix}
$$

$$
= \begin{bmatrix} 1.7 & 0.65 \\ 3.0 & 1.15 \end{bmatrix}
$$

$$
\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}
$$

$$
= \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix} \cdot \begin{bmatrix} 1.0 & 1.5 \\ 2.0 & 2.5 \end{bmatrix}
$$

$$
= \begin{bmatrix} 7.0 & 9.0 \\ 10.0 & 13.0 \end{bmatrix}
$$

$$
\frac{\partial L}{\partial b} = \sum\left(\begin{bmatrix} 1.0 & 1.5 \\ 2.0 & 2.5 \end{bmatrix}, \text{axis}=0\right) = [3.0, 4.0]
$$

**代码实现**（来自 `common/layers.py`）：
```python
class Affine:
    def __init__(self, W, b):
        self.W = W  # 权重矩阵，shape: (input_features, output_features)
        self.b = b  # 偏置向量，shape: (output_features,)
        # 用于保存输入数据，方便反向传播计算梯度
        self.x = None
        self.original_x_shape = None  # 保存原始输入形状，用于反向传播时恢复
        # 用于保存梯度，方便梯度下降法更新参数
        self.dW = None  # 权重梯度
        self.db = None  # 偏置梯度

    def forward(self, x):
        """
        前向传播：计算 out = x·W + b
        参数：
            x: 输入数据，shape 为 (batch_size, ..., input_features)
               可以是多维张量，会自动展平为二维矩阵
        返回：
            out: 输出数据，shape 为 (batch_size, output_features)
        """
        # 保存原始形状，用于反向传播时恢复 dx 的形状
        self.original_x_shape = x.shape
        # 将输入展平为二维矩阵：(batch_size, total_features)
        self.x = x.reshape(x.shape[0], -1)
        # 执行仿射变换：矩阵乘法 + 偏置
        out = np.dot(x, self.W) + self.b
        return out

    def backward(self, dout):
        """
        反向传播：根据链式法则计算各参数的梯度
        参数：
            dout: 上游（后一层）传来的梯度，shape 为 (batch_size, output_features)
        返回：
            dx: 输入的梯度，shape 与原始输入 x 相同
        """
        # 计算输入梯度：dx = dout·Wᵀ
        dx = np.dot(dout, self.W.T)
        # 恢复为原始输入形状
        dx = dx.reshape(*self.original_x_shape)

        # 计算权重梯度：dW = xᵀ·dout
        self.dW = np.dot(self.x.T, dout)

        # 计算偏置梯度：db = sum(dout, axis=0)
        self.db = np.sum(dout, axis=0)

        return dx
```

**代码说明**：
- **参数管理**：在构造函数中接收权重 W 和偏置 b
- **形状处理**：支持多维输入，自动展平为二维矩阵
- **梯度计算**：
  - dx = dout @ Wᵀ：传给前一层
  - dW = xᵀ @ dout：用于更新权重
  - db = sum(dout)：用于更新偏置
- **注意事项**：保存 `self.x` 和 `self.original_x_shape` 供反向传播使用

---

## 3.6 输出层的反向传播和实现

### 3.6.1 Softmax-with-Loss 层

**为什么要组合 Softmax 和 Loss**：
- Softmax 和交叉熵损失函数经常一起使用
- 分开计算反向传播较复杂
- 组合后的反向传播非常简洁

**正向传播**：

```
1. Softmax: y = softmax(a)
   yᵢ = exp(aᵢ) / Σⱼ exp(aⱼ)

2. Cross-Entropy Loss: L = -Σᵢ tᵢ log(yᵢ)
   其中 t 是 one-hot 标签
```

### 3.6.2 Softmax-with-Loss 的反向传播

**神奇的简洁公式**：

```
∂L/∂a = y - t
```

其中：
- a: Softmax 的输入（logits）
- y: Softmax 的输出（概率分布）
- t: one-hot 真实标签

**重要特性**：
- 反向传播的结果极其简洁
- 直接是预测概率与真实标签的差
- 这个性质使得训练非常高效

**直观理解**：

假设三分类问题：
$$
真实标签: t = [0, 1, 0] \text{ (第2类)}
$$

$$
预测概率: y = [0.3, 0.6, 0.1]
$$

$$
梯度: \frac{\partial L}{\partial a} = y - t = [0.3, -0.4, 0.1]
$$

- 第1类：预测0.3，应该是0，梯度为正，会降低这个输出
- 第2类：预测0.6，应该是1，梯度为负，会提高这个输出
- 第3类：预测0.1，应该是0，梯度为正，会降低这个输出

**批量处理**：

对于批量数据：
```
Y: (batch_size, num_classes)
T: (batch_size, num_classes)

∂L/∂A = (Y - T) / batch_size
```

除以 batch_size 是为了计算平均损失的梯度。

**数学推导详解**：

在输出层，我们一般使用 Softmax 作为激活函数。

**对于 Softmax 函数**：

$$
y_k = \frac{e^{a_k}}{\sum_{i=1}^{n} e^{a_i}} \quad k = 1 \sim n
$$

**其导数为**：

$$
\frac{\partial y_k}{\partial x_i} = \begin{cases}
y_k(1 - y_k), & k = i \\
-y_k y_i, & k \neq i
\end{cases}
$$

**重要说明**：
- 当 k = i 时（对自身求导）：导数为 yₖ(1 - yₖ)
- 当 k ≠ i 时（对其他输出求导）：导数为 -yₖyᵢ
- 这是因为 Softmax 的每个输出都依赖于所有输入

**为什么要组合 Softmax-with-Loss 层**：

而对于输出层，一般会直接将结果代入损失函数的计算。对于我们之前介绍的分类问题，这里选择交叉熵误差（Cross Entropy Error）作为损失函数。就可以得到一个 **Softmax-with-Loss 层**，它包含了 Softmax 和 Cross Entropy Loss 两部分。

**导数的计算会比较复杂**，可以用下图表示如下：

（此处可插入复杂的计算图）

**但是，组合后的结果却非常简洁**：

设 Softmax 输入为 a，输出为 y，损失为 L：

$$
L = -\sum_i t_i \log(y_i)
$$

$$
y_i = \frac{\exp(a_i)}{\sum_j \exp(a_j)}
$$

$$
\frac{\partial L}{\partial a_k} = \frac{\partial L}{\partial y_k} \times \frac{\partial y_k}{\partial a_k} + \sum_{i \neq k} \frac{\partial L}{\partial y_i} \times \frac{\partial y_i}{\partial a_k} = y_k - t_k
$$

（完整推导涉及 Softmax 的雅可比矩阵，此处省略）

**代码实现**（来自 `common/layers.py`）：
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None  # 保存损失值
        self.y = None     # 保存 Softmax 输出（概率分布）
        self.t = None     # 保存真实标签

    def forward(self, x, t):
        """
        前向传播：计算 Softmax 概率和交叉熵损失
        参数：
            x: 输入数据（logits），shape 为 (batch_size, num_classes)
            t: 真实标签，可以是独热编码 (batch_size, num_classes) 或整数标签 (batch_size,)
        返回：
            loss: 交叉熵损失值（标量）
        """
        self.t = t
        # Step 1: Softmax 将 logits 转换为概率分布
        self.y = softmax(x)
        # Step 2: 计算与真实标签的交叉熵损失
        self.loss = cross_entropy(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        """
        反向传播：计算输入的梯度
        参数：
            dout: 上游梯度，对于损失函数通常为 1
        返回：
            dx: 输入 x 的梯度，shape 与 x 相同

        数学推导（为什么 dx = y - t）：
            设 L = -log(y_k)，其中 y = softmax(x)，k 是正确类别
            对于正确类别 k：∂L/∂x_k = y_k - 1
            对于其他类别 j：∂L/∂x_j = y_j
            合并表示：∂L/∂x = y - t（t 是独热向量）
        """
        batch_size = self.t.shape[0]

        # 根据标签格式选择计算方式
        if self.t.size == self.y.size:
            # 独热编码格式：t.shape == y.shape，例如 t = [[1,0,0], [0,1,0]]
            # 直接向量化计算：dx = (y - t) / batch_size
            dx = (self.y - self.t) / batch_size
        else:
            # 整数标签格式：t 是一维数组，例如 t = [0, 1]
            # 需要通过索引操作实现 y - t 的效果
            dx = self.y.copy()
            # np.arange(batch_size) = [0, 1, ..., batch_size-1] 生成样本索引
            # self.t = [0, 1] 是每个样本的正确类别索引
            # dx[np.arange(batch_size), self.t] 访问每个样本正确类别位置的值
            dx[np.arange(batch_size), self.t] -= 1
            dx /= batch_size

        return dx
```

**代码说明**：
- **组合层设计**：将 Softmax 和交叉熵组合为一层，简化梯度计算
- **前向传播**：依次执行 Softmax 和交叉熵计算
- **反向传播**：使用简洁公式 dx = (y - t) / batch_size
- **支持两种标签**：
  - 独热编码：直接相减
  - 整数标签：通过索引操作实现
- **批量平均**：除以 batch_size 使梯度大小与批量大小无关

---

## 3.7 反向传播的综合代码实现

### 3.7.1 神经网络层的统一接口

**层的基本结构**：

每个层需要实现两个方法：
1. `forward(x)`: 正向传播，计算输出
2. `backward(dout)`: 反向传播，计算梯度

**接口设计**：

```python
class Layer:
    def forward(self, x):
        """
        正向传播
        
        参数:
            x: 输入数据
        
        返回:
            out: 输出数据
        """
        pass
    
    def backward(self, dout):
        """
        反向传播
        
        参数:
            dout: 上游传来的梯度 (∂L/∂out)
        
        返回:
            dx: 传给下游的梯度 (∂L/∂x)
        """
        pass
```

### 3.7.2 各层的实现

各层的详细实现已在前面的章节中介绍，所有代码均来自 `common/layers.py`：

- **ReLU 层**：详见 3.4.1 节
- **Sigmoid 层**：详见 3.4.2 节
- **Affine 层**：详见 3.5.4 节
- **Softmax-with-Loss 层**：详见 3.6.2 节

每个层都实现了两个核心方法：
- `forward(x)`: 前向传播，计算输出并保存必要的中间值
- `backward(dout)`: 反向传播，计算梯度并传递给下一层

### 3.7.3 完整的两层神经网络（使用反向传播）

**网络结构**：

```
输入层 → Affine1 → ReLU → Affine2 → Softmax-with-Loss
```

**代码实现**（来自 `ch04_backward/two_layer_net.py`）：
```python
import numpy as np
from common.functions import *
from common.gradient import numerical_gradient
from common.layers import *
from collections import OrderedDict  # 有序字典，用来保存层结构

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 初始化权重和偏置
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b1'] = np.zeros((1, hidden_size))
        self.params['b2'] = np.zeros((1, output_size))
        
        # 定义层结构（使用有序字典保证顺序）
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()

    # 前向传播
    def forward(self, x):
        # 对于每一层，依次调用forward方法
        for layer in self.layers.values():
            x = layer.forward(x)
        return x

    # 计算损失
    def loss(self, x, t):
        y = self.forward(x)
        loss = self.lastLayer.forward(y, t)
        return loss

    # 计算准确度
    def accuracy(self, x, t):
        y = self.forward(x)  # 预测分类数值
        # 根据最大概率得到分类号
        y = np.argmax(y, axis=1)
        # 与正确的标签对比，获得准确率
        accuracy = np.sum(y==t) / x.shape[0]
        return accuracy

    # 计算梯度（数值微分法，用于梯度检验）
    def numerical_gradient(self, x, t):
        loss_f = lambda _: self.loss(x, t)
        grads = {}
        grads['W1'] = numerical_gradient(loss_f, self.params['W1'])
        grads['W2'] = numerical_gradient(loss_f, self.params['W2'])
        grads['b1'] = numerical_gradient(loss_f, self.params['b1'])
        grads['b2'] = numerical_gradient(loss_f, self.params['b2'])
        return grads
    
    # 计算梯度（反向传播法）
    def gradient(self, x, t):
        # 前向传播计算损失
        self.loss(x, t)
        
        # 反向传播计算梯度
        dy = 1
        dy = self.lastLayer.backward(dy)
        
        # 将所有层反向处理
        layers = list(self.layers.values())
        for layer in reversed(layers):
            dy = layer.backward(dy)
        
        # 提取各层的梯度
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['W2'] = self.layers['Affine2'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['b2'] = self.layers['Affine2'].db
        return grads
```

**代码说明**：
- **OrderedDict**：使用有序字典保存层，确保按顺序执行
- **层化设计**：每个层是独立的对象，具有 forward 和 backward 方法
- **前向传播**：循环调用各层的 forward 方法
- **反向传播**：倒序调用各层的 backward 方法
- **梯度提取**：从 Affine 层中获取权重和偏置的梯度
- **两种梯度方法**：
  - `numerical_gradient`：数值微分，用于梯度检验
  - `gradient`：反向传播，用于实际训练

### 3.7.4 训练流程

**完整训练代码**（来自 `ch04_backward/1_digit_recognizer_nn_train.py`）：
```python
import numpy as np
import pandas as pd
from common.load_data import get_data
from two_layer_net import TwoLayerNet

# 1. 加载数据
x_train, y_train, x_test, y_test = get_data()

# 2. 初始化模型
model = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 3. 设置超参数
learning_rate = 0.1
num_epochs = 20
batch_size = 100
train_size = x_train.shape[0]
iter_per_epoch = train_size // batch_size
iter_num = num_epochs * iter_per_epoch

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 4. 循环迭代，用梯度下降法训练模型
for i in range(iter_num):
    # 4.1 随机选取批量数据
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = y_train[batch_mask]
    
    # 4.2 计算梯度（反向传播）
    grad = model.gradient(x_batch, t_batch)
    
    # 4.3 更新参数
    for key in ('W1', 'W2', 'b1', 'b2'):
        model.params[key] -= learning_rate * grad[key]
    
    # 4.4 保存当前训练损失
    train_loss_list.append(model.loss(x_batch, t_batch))
    
    # 4.5 每个epoch结束时计算准确度
    if i % iter_per_epoch == 0:
        train_acc = model.accuracy(x_train, y_train)
        test_acc = model.accuracy(x_test, y_test)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

# 5. 画图
import matplotlib.pyplot as plt
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc')
plt.legend(loc='best')
plt.show()
```

**代码说明**：
- **数据加载**：使用 `get_data()` 从 `common/load_data.py` 加载 MNIST 数据
- **模型初始化**：784(输入) → 50(隐藏) → 10(输出)
- **超参数设置**：学习率=0.1，20个Epoch，Batch=100
- **训练循环**：
  - 使用 **反向传播** 计算梯度（比数值微分快得多）
  - SGD 更新参数
  - 记录损失和准确率
- **可视化**：绘制训练和测试准确率曲线

**效率对比**：
- 数值微分法：每次迭代需要 2×参数数 次前向传播
- 反向传播法：每次迭代只需 1 次前向 + 1 次反向
- **速度提升：数百倍至数千倍！**

---

## 3.8 梯度检查

### 3.8.1 为什么需要梯度检查

**反向传播的复杂性**：
- 反向传播的实现容易出错
- 公式推导复杂，矩阵转置和维度容易搞混
- 错误的梯度可能导致训练无法收敛

**梯度检查的作用**：
- 验证反向传播实现是否正确
- 对比数值梯度和解析梯度
- 调试神经网络的重要工具

### 3.8.2 梯度检查的方法

**基本思想**：
- 使用数值微分计算梯度（数值梯度）
- 使用反向传播计算梯度（解析梯度）
- 比较两者是否接近

**数值梯度**（慢但可靠）：
```
∂f/∂x ≈ (f(x + h) - f(x - h)) / (2h)
```

**解析梯度**（快但可能有bug）：
- 通过反向传播计算

**比较方法**：

计算相对误差：
$$
相对误差 = \frac{|数值梯度 - 解析梯度|}{|数值梯度| + |解析梯度|}
$$

**判断标准**：
- 相对误差 < 1e-7: 非常好
- 相对误差 < 1e-5: 可以接受
- 相对误差 < 1e-3: 需要检查
- 相对误差 > 1e-3: 很可能有bug

### 3.8.3 梯度检查的实现

**代码实现**：
```python
def gradient_check(network, x, t, epsilon=1e-7):
    """
    梯度检查：对比数值梯度和解析梯度
    
    参数:
        network: 神经网络对象
        x: 输入数据
        t: 真实标签
        epsilon: 判断阈值
    
    返回:
        是否通过检查
    """
    # 计算数值梯度
    numerical_grads = network.numerical_gradient(x, t)
    
    # 计算解析梯度（反向传播）
    analytical_grads = network.gradient(x, t)
    
    # 对比每个参数的梯度
    for key in numerical_grads.keys():
        num_grad = numerical_grads[key]
        ana_grad = analytical_grads[key]
        
        # 计算相对误差
        diff = np.abs(num_grad - ana_grad)
        denom = np.abs(num_grad) + np.abs(ana_grad) + 1e-7
        relative_error = np.max(diff / denom)
        
        print(f"{key}: 相对误差 = {relative_error:.2e}")
        
        if relative_error > epsilon:
            print(f"  警告: {key} 的梯度可能存在错误!")
            return False
    
    print("梯度检查通过!")
    return True
```

**使用示例**：
```python
# 创建网络
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 准备少量测试数据
x = x_train[:10]  # 取10个样本
t = y_train[:10]

# 执行梯度检查
gradient_check(network, x, t)
```

**重要提示**：
- 梯度检查计算量大，只用于调试
- 训练时使用反向传播，不使用数值梯度
- 只需在网络实现完成后检查一次

---

## 本章小结

### 核心概念

1. **计算图**：
   - 将计算过程可视化为图结构
   - 支持正向传播和反向传播
   - 局部计算，高效计算梯度

2. **链式法则**：
   - 复合函数求导的基本规则
   - 反向传播的数学基础
   - 梯度逐层传递

3. **反向传播**：
   - 高效计算神经网络梯度的算法
   - 每个节点根据局部导数传递梯度
   - 相比数值微分，速度提升显著

4. **各层的反向传播**：
   - 加法层：直接传递梯度
   - 乘法层：交换输入值
   - ReLU层：根据mask选择性传递
   - Sigmoid层：y(1-y)
   - Affine层：矩阵乘法和转置
   - Softmax-with-Loss层：y - t（极其简洁）

### 实现要点

1. **层的统一接口**：
   - forward: 正向传播，保存必要的中间值
   - backward: 反向传播，计算并传递梯度

2. **梯度检查**：
   - 对比数值梯度和解析梯度
   - 验证反向传播实现正确性
   - 只在调试时使用

3. **效率提升**：
   - 反向传播比数值微分快得多
   - 使得深度神经网络训练成为可能
   - 是深度学习的核心算法

### 下一章预告

下一章将学习：
- 参数更新的优化算法（SGD、Momentum、Adam等）
- 权重初始化方法
- Batch Normalization
- 正则化技术（Dropout、L2正则化）
- 超参数调优方法
