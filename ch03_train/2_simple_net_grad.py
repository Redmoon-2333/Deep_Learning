import numpy as np

from common.functions import *
from common.gradient import numerical_gradient

# =============================================================================
# SimpleNet: 最简单的神经网络，用于演示梯度计算流程
# 网络结构: 输入层(2个神经元) -> 输出层(3个神经元)
# =============================================================================
class SimpleNet:
    def __init__(self):
        # 初始化权重矩阵 W，形状为 (2, 3)
        # 2: 输入特征数，3: 输出类别数
        self.W = np.random.randn(2, 3)
    
    def forward(self, x):
        # 前向传播：计算预测概率分布
        a = x @ self.W    # 矩阵乘法：计算加权和
        y = softmax(a)    # 转换为概率分布
        return y
    
    def loss(self, x, t):
        # 计算损失值：预测与真实标签的交叉熵
        y = self.forward(x)
        loss = cross_entropy(y, t)
        return loss


# =============================================================================
# 主流程：完整的梯度计算示例
# =============================================================================
if __name__ == '__main__':
    
    # -------------------------------------------------------------------------
    # 第1步：准备数据
    # -------------------------------------------------------------------------
    x = np.array([0.6, 0.9])   # 输入数据：2个特征
    t = np.array([0, 0, 1])    # 真实标签：one-hot编码，真实类别为2
    
    # -------------------------------------------------------------------------
    # 第2步：创建神经网络
    # -------------------------------------------------------------------------
    net = SimpleNet()
    # 假设初始化后的权重为：
    # net.W = [[ 0.5, -0.3,  0.8],
    #          [ 0.2,  0.7, -0.4]]
    
    # -------------------------------------------------------------------------
    # 第3步：计算梯度
    # -------------------------------------------------------------------------
    # 将损失函数包装成只关于W的函数
    # 注意：lambda参数w看似未用，但numerical_gradient内部会直接修改net.W
    f = lambda w: net.loss(x, t)
    
    # 调用数值梯度计算
    gradw = numerical_gradient(f, net.W)
    
    print(gradw)


# =============================================================================
# 完整计算流程示例（假设权重如上）：
# =============================================================================
#
# 【前向传播】
# 输入: x = [0.6, 0.9]
# 权重: W = [[ 0.5, -0.3,  0.8],
#            [ 0.2,  0.7, -0.4]]
#
# a = x @ W
#   = [0.6*0.5 + 0.9*0.2,  0.6*(-0.3) + 0.9*0.7,  0.6*0.8 + 0.9*(-0.4)]
#   = [0.3 + 0.18,         -0.18 + 0.63,          0.48 - 0.36]
#   = [0.48,               0.45,                  0.12]
#
# y = softmax([0.48, 0.45, 0.12])
#   = [0.370, 0.359, 0.271]  # 三个类别的预测概率
#
# 【计算损失】
# 真实标签: t = [0, 0, 1] → 真实类别是2
# 类别2的预测概率: y[2] = 0.271
# loss = -log(0.271) = 1.306
#
# 【计算梯度】以 W[0,0] 为例：
# h = 0.0001
#
# W[0,0] = 0.5 + 0.0001 = 0.5001 → 重新前向传播 → loss1 = 1.3058
# W[0,0] = 0.5 - 0.0001 = 0.4999 → 重新前向传播 → loss2 = 1.3062
#
# grad[0,0] = (loss1 - loss2) / (2h)
#           = (1.3058 - 1.3062) / 0.0002
#           = -0.02
#
# 意义：W[0,0]增加时，损失减小，所以梯度为负
#       梯度下降时：W[0,0] = W[0,0] - lr * grad[0,0]
#       由于grad为负，所以W[0,0]会增大，损失继续减小
#
# 【最终梯度矩阵】
# gradw = [[∂L/∂W₀₀, ∂L/∂W₀₁, ∂L/∂W₀₂],
#          [∂L/∂W₁₀, ∂L/∂W₁₁, ∂L/∂W₁₂]]
# 每个元素表示: 该权重变化对损失的影响程度
# =============================================================================
